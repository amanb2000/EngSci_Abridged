\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools} 
\usepackage{bbm}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\inpr{\langle}{\rangle}%

\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

% \def\reals{{\rm I\!R}}
\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}


\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{EE/Ma/CS 126a: Information Theory}
\author{Aman Bhargava}
\date{October-December 2022}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

This document offers an overview of EE/Ma/CS 126a at Caltech. They comprise my condensed course notes for the course. No promises are made relating to the correctness or completeness of the course notes. These notes are meant to highlight difficult concepts and explain them simply, not to comprehensively review the entire course.

\paragraph{Course Information}
\begin{itemize}
\item Professor: Michelle Effros
\item Term: 2022 Fall
\end{itemize}





\chapter{Math Review}


\section{Combinatorics \& Probability}

\paragraph{Binomial Distribution \& Coefficient} 

\begin{itemize}

\item \textbf{Bernoulli Process}: Repeated trials, each with one binary outcome.
The probability of a positive outcome is $p\in [0,1]$. Each trial is
independent.
\item \textbf{Binomial Distribution}: Let $x$ represent the number of successful
trials in a Bernoulli process repeated $n$ times with success probability $p$.
The binomial distribution gives the probability distribution on $x$: 

\begin{equation}\label{eqn:binomial}
b(x; n, p) = {n \choose k} p^x (1-p)^{n-x}
\end{equation}

Which has $\mu = np$, $\sigma^2 = npq$.

\item \textbf{Intuition for Binomial Distribution}: The probability of observing
a sequence with $x$ positive outcomes and $n-x$ negative outcomes is $p^x
(1-p)^{n-x}$. There are ${n\choose k}$ different sequences (i.e., permutations)
that have $x$ positive cases and $n$ negative cases. Thus the total probability
of observing $x$ positive cases is given by Eq~\ref{eqn:binomial}.

\item \textbf{Binomial Coefficient}: 
\begin{equation}
\label{eqn:binomial_coeff}
{n\choose k} = \frac{n!} {k! (n-k)!}
\end{equation}

\end{itemize}


\section{Logarithm Identities}

Entropy calculations and manipulations involve a lot of logarithms. They're not
so bad once you get to know them, though: 
\begin{itemize}
\item \textbf{Definition}: $$a = b^{\log_b a}$$
\item \textbf{Sum-Product}: $$\log_c(ab) = \log_c a + \log_c b$$
\item \textbf{Difference-Quotient}: $$\log (a/c) = \log a - \log c$$ 
$$\log \frac 1 a = - \log a$$
\item \textbf{Product-Exponent}: $$\log_c(a^n) = n\log_c(a)$$
\item \textbf{Swapping Base}: $$\log_b(a) = \log_a(b)$$
\item \textbf{Swapping Exponential}: $$a^{\log n} = n^{\log a}$$
\item \textbf{Change of Base}; $$\log_b(a) = \frac{\log_x(a)}{\log_x(b)}$$
\end{itemize}















\chapter{Entropy Definitions}
\textit{Chapter 2 of Elements of Information Theory}.


\section{Entropy, Conditional Entropy, Joint Entropy}

\paragraph{Entropy Definition (Discrete): } 
\begin{align}\label{eqn:entropy}
H(X) &= \sum_{x\in \mathcal X} p(x) \log ( \frac{1}{p(x)} ) \\
&= - \sum_{x\in \mathcal X} p(x) \log p(x) \\ 
&= \mathbb{E} [\log \frac {1} {p(x)}] 
\end{align}


\begin{theorem}{Properties of Entropy}
\begin{enumerate}
\item \textbf{Non-negativity: } $H(X) \geq 0$ -- \textit{Reasoning: Entropy is the sum-product of
non-negative terms}.
\item \textbf{Change of base: } $H_b(X) = (\log_b a) H_a(X)$

\item \textbf{Bernoulli entropy: } $H(X) = -p\log p - q \log q \equiv H(p)$.
\begin{itemize}
\item $H(p)$ is a concave function of $p$, peaks at $p=q=0.5$.
\end{itemize}
\end{enumerate}
\end{theorem}



\paragraph{Joint Entropy: } Literally just entropy of vector $[X,Y]^\top$.
\begin{align}
\label{eqn:joint_entropy}
H(X, Y) &= - \sum_{x\in \mathcal X} \sum_{y\in \mathcal Y}^{} p(x,y) \log p(x,y) \\ 
&= \mathbb E [\log p(x,y)] \\
\end{align}


\paragraph{Conditional Entropy: } $H(Y|X)$ is the expected entropy of $p(y|x)$
averaged across all $x$.
\begin{align}
\label{eqn:conditional_entropy}
H(Y | X) &= \sum_{x\in \mathcal X} p(x) \underbrace{ \sum_{y\in \mathcal Y} p(y
| x) \log p(y | x) }_{H(Y| X = x)} \\
&= - \mathbb E [\log p(Y | X)]
\end{align}



Entropy can be thought of as the \textbf{uncertainty} in the value of a random
variable. High entropy corresponds to a high degree of uncertainty. Conditional
entropy $H(Y|X)$ can be thought of as the average \textbf{remaining uncertainty}
in the value of $Y$ after learning the value of $X$.


\begin{theorem}{Chain Rule for Entropy}
\begin{align}\label{eqn:simple_entropy_chain}
H(X, Y) &= H(X) + H(Y | X) \\
&= H(Y) + H(X | Y)
\end{align}

It also follows that 
\begin{align}
H(X, Y | Z) &= H(X | Z) + H(Y | X, Z) 
\end{align}


\textbf{Proof sketch:} 
\begin{itemize}
\item Recall that $H(X) = -\mathbb E [\log p(x)]$ and $H(Y | X) = -\mathbb E
[\log p(y | x)]$. 
\item $\log p(x) + \log p(y | x) = \log(p(x) \cdot p(y | x)) = \log p(x,y)$.
\item The proof follows from there. You can also write out the full sum form of
$H(X, Y)$ and recover $H(X), H(Y| X)$ from there if you're feeling rigorous.
\end{itemize}
\end{theorem}





\section{Relative Entropy \& Mutual Information}

\paragraph{Relative Entropy: } $D(p \| q)$ gives a \textit{distance} between
distributions $p(x)$ and $q(x)$. Also known as KL divergence. 
\begin{align}
\label{eq:relative_entropy}
D(p \| q) &= \sum_{x\in \mathcal X} p(x) \log \frac{p(x)}{q(x)} \\ 
&= \mathbb E_{p(x)} [\log \frac {p(x)}{q(x)}] \\ 
\end{align}

This also corresponds to the \textbf{inefficiency} of using $q$ as a replacement
for $p$ when generating codes for tokens drawn from $p(x)$. 
\begin{itemize}
\item \textbf{Average code length with correct $p(x)$:} $H(p)$.
\item \textbf{Average code length with incorrect $q(x)$:} $H(p) + D(p \| q)$.
\end{itemize}

\begin{theorem}{Properties of Relative Entropy}
\begin{enumerate}
\item \textbf{Asymmetric: } In general, $D(p\| q) \neq D(q \| p)$.
\item \textbf{Non-negative: } $D(p\| q) \geq 0$.
\item \textbf{Identity: } If $D(p\| q) = 0$ then $p \equiv q$.
\end{enumerate}
\end{theorem}



\paragraph{Conditional Relative Entropy/KL Divergence: } Distance between two
distributions when conditioned on the same variable. Similar idea of averaging
across all values of the conditioning variable. 
\begin{align}\label{eqn:conditional_relative_entropy}
D(p(y|x) \| q(y | x)) &= \sum_{x\in \mathcal X}^{} p(x) \big[ \sum_{y\in \mathcal
Y}^{} p(y|x) \log \frac{p(y | x)}{q(y|x)}\big] \\
&= \mathbb E_{p(x,y)} \log \big[ \frac{p(y|x)}{q(y|x)}\big]
\end{align}







We now move onto \textbf{mutual information} -- a measure of the dependence of
two variables. As we will see, it is the \textbf{reduction in uncertainty} of
$X$ due to knowing $Y$, on average. 

\paragraph{Mutual Information: } \begin{align}
\label{eqn:mutual_info}
I(X; Y) &= \sum_{x\in \mathcal X}^{} \sum_{y\in \mathcal Y}^{} p(x, y) \log
(\frac{p(x,y)}{p(x)p(y)} ) \\ 
&= D\big(p(x,y) \| p(x)p(y) \big) \\
&= \mathbb E [\log \frac{p(X,Y)}{p(X)p(Y)}]
\end{align}




\begin{theorem}{Properties of Mutual Information}
\begin{itemize}
\item It is the \textbf{divergence} between $p(x,y)$ and $p(x)p(y)$.
\item \textbf{Symmetry: } $I(X; Y) = I(Y; X)$.
\item \textbf{Relation to Entropy: } Mutual information is the \textit{reduction
in uncertainty} of each RV expected after discovering the other variable's
value. 
	\begin{align}
		\label{eqn:mi_entropy}
		I(X; Y) &= H(X) - H(X | Y) \\ 
				&= H(Y) - H(Y | X) \\
	\end{align}

\item \textbf{Alternative Entropy Relation: }
	\begin{align}
		\label{eqn:mi_entropy_2}
		I(X; Y) &= H(X) + H(Y) - H(X, Y) \\ 
		I(X; X)	&= H(X) - H(X | X) \\ 
				&= H(X)
	\end{align}

\end{itemize}

\textbf{Proof Sketch for (3): } 
\begin{itemize}
\item Within the definition of $I(X; Y)$ there is a term $\log \frac{p(x,y)}{p(x)p(y)}$. 
\item Once you convert the argument of the log into $p(x | y) / p(x)$, you can separate out $H(X) - H(X | Y)$ using the
quotient-difference logarithm rule.
\end{itemize}
\end{theorem}


\paragraph{Conditional Mutual Information: } $I(X, Y | Z)$ is the average
reduction in uncertainty on the value of $X$ due to knowing $Y$ when $Z$ is
given. 
\begin{align}
	\label{eqn:cond_MI}
	I(X; Y | Z) &= H(X | Z) - H(X | Y, Z) \\ 
				&= \mathbb E_{p(x,y,z)}	\log \big[ \frac{p(X, Y | Z)}{p(X|Z)
				p(Y|Z)} big] \\
\end{align}








\section{Chain Rules: $H(\cdot), I(\cdot ; \cdot)$}

These chain rules end up being very useful in a lot of proofs. Deeply
understanding them is a good idea. 

\begin{theorem}{Entropy chain rule}
\label{thm:entropy_chain_rule}
Let $X_1, X_2, \dots, X_n \sim p(x_1, x_2, \dots, x_n)$. Then 
\begin{align}
	\label{eqn:entropy_chain_rule}
	H(X_1, \dots, X_n) &= \sum_{i=1}^{n} H(X_i | X_{i-1}, X_{i-2}, \dots, X_1)
\end{align}
\textbf{Proof: } Repeatedly apply Equation~\ref{eqn:simple_entropy_chain}.
\end{theorem}

\paragraph{Intuition of Chain Rule: } It's important to note that the term in
the sum is conditioned on elements $X_j$ with $j < i$. Conditioning always
reduces entropy, so it's as though the ``additional entropy'' from the term must
be reduced to account for the previous terms already having been added to the
total. Also note that any order can suffice -- there is no absolute order in the
sum.



\begin{theorem}{Chain Rule for Mutual Information}
\begin{align}
	\label{eqn:chain_rule_MI}
	I(X_1, \dots, X_n; Y) &= \sum_{i=1}^{n} I(X_i; Y | X_{i-1}, X_{i-2}, \dots,
	X_1) \\
\end{align}
\textbf{Proof: } Start with the entropy definition of mutual information. Then
apply the chain rule for entropy (Theorem~\ref{thm:entropy_chain_rule}).
\begin{itemize}
\item $I(X_{1:n} ; Y) = H(X_{1:n}) - H(X_{1:n} | Y)$.
\item $= \sum_{i=1}^{n} H(X_i | X_{i-1:1}) - \sum_{i=1}^{n} H(X_i | X_{i-1:1},
Y)$.
\item $= \sum_{i=1}^{n} I(X_i; Y | X_{1:i-1})$.
\end{itemize}
\end{theorem}





\begin{theorem}{Chain Rule for Relative Entropy}
\begin{equation}
D(p(x,y) \| q(x,y)) = D(p(x) \| q(x)) + D(p(y | x) \| q(y|x))
\end{equation}

\textbf{Proof sketch:} Expand the LHS in log-sum form. Separate the term in the
$\log$ into a sum of two $\log$ terms corresponding to the two divergences on
the RHS. 

\end{theorem}



\section{Jensen's Inequality \& Consequences}

\begin{theorem}{Jensen's Inequality (and Convexity)}
For any convex function $f$ and random variable $X$, 
\begin{align}
	\mathbb E \big[ f(x)\big] \geq f(\mathbb E[X])
\end{align}

Where ``convex $f$ in $(a,b)$'' $\iff$ 
\begin{align}
	\label{eqn:convexity}
	f(\lambda x_1 + (1-\lambda) x_2 ) \leq \lambda f(x_1) + (1-\lambda) f(x_2)
\end{align}
for all $x_1, x_2 \in (a, b)$ and $\lambda \in [0,1]$. \textbf{Strict} convexity
has equality iff $\lambda \in \{0,1\}$. \textbf{Concave} $f$ $\iff$ convex $-f$.


\textbf{Proof sketch of Jensen's Inequality: } 
\begin{itemize}
\item Start with $|\mathcal X| = 2$. Then we can let a vector $\mathbf p\in
\mathbb R^2$ represent f(x). 
\item $\mathbb E[f(x)] = p_1 f(x_1) + p_2 f(x_2)$. 
\item $f(\mathbb E[X]) = f(p_1 x_1 + p_2 x_2)$.
\item Show equivalence of Jensen's inequality $\iff$ $\mathbf p$ is a valid PMF.
\item Expand to $|\mathcal X| = k-1$ (induction proof).
\item Use continuity arguments for continuous case.
\end{itemize}
\end{theorem}


\begin{theorem}{Implications of Jensen's Inequality}
\begin{enumerate}
\item \textbf{Information Inequality: } $D(p\| q) \geq 0$.
\item \textbf{MI Inequality: } $I(X; Y) \geq 0$.
\item \textbf{Maximum Entropy: } $H(X) \leq \log |\mathcal X|$. Maximum is
achieved by $X \sim {uniform} (\mathcal X)$.
\item \textbf{Information can't Hurt: } $H(X|Y) \leq H(X)$.
\item \textbf{Entropy Sum Bound: } $H(X_{1:n}) \leq \sum_{i=1}^{n} H(X_i)$ with
equality for independent $X_i$.
\end{enumerate}
\end{theorem}



\section{Log-Sum Inequality \& Consequences}

\begin{theorem}{Log-Sum Inequality}
Let $\{a_i, b_i\}_{i=1}^n$ be \textbf{non-negative} numbers. Then 
\begin{align}
\sum_{i=1}^{n} a_i \log \frac{a_i}{b_i} \geq \big(\sum_{i=1}^{n} a_i \big) \log
\frac{\sum a_i}{\sum b_i} 
\end{align}
With equality iff $\frac{a_i}{b_i}$ is constant.

\textbf{Proof sketch: } 
\begin{enumerate}
\item Introduce $\alpha_i = a_i/\sum a_j$ and $t_i = a_i/b_i$. $\alpha_i$ values
comprise a probability distribution (sum to 1). 
\item Apply \textbf{Jensen's} to $\sum \alpha_i f(t_i) \geq f(\sum \alpha_i
t_i)$ where $f() = \log()$.
\end{enumerate}
\end{theorem}




\begin{theorem}{Applications of Log-Sum Inequality}

\begin{itemize}
\item \textbf{Convexity of Relative Entropy: } $D(p\|q)$ is convex in pairs of
$p, q$.
	\begin{align}
		D(\lambda p_1 + (1-\lambda) p_1 \| \lambda q_1 + (1-\lambda) q_2) \leq
		\lambda D(p_1 \| q_1) + (1-\lambda) D(p_2 \| q_2)
	\end{align}

\item \textbf{Concavity of Mutual Information: } 
\begin{enumerate}
\item \textbf{For fixed $p(y|x)$:} $I(X; Y)$ is concave in $p(x)$.
\item \textbf{For fixed $p(x)$:} $I(X; Y)$ if concave in $p(y | x)$.
\item \textit{This property is really handy when doing channel capacity
calculations/bounds!} 
\end{enumerate}
\end{itemize}


\end{theorem}









\section{Data Processing Inequality}

\begin{theorem}{Data Processing Inequality}
Let $X \to Y \to Z$ form a markov chain. That is, $p(x,y,z) = p(x)p(y|x)p(z|y)$.
Then 
\begin{equation}
I(X; Y) \geq I(X; Z)
\end{equation}

That is, the mutual information between $X, Z$ is bounded by the mutual
information between $X, Y$.

\textbf{Proof sketch:} 
\begin{itemize}
\item Expand $I(X; Y, Z)$ with the chain rule.
\item ($\star$) Observe that $I(X; Z | Y) = 0$ since $X, Z$ are \textbf{conditionally
independent} given $Y$ (recall Markov theory -- head-to-tail connection).
\item Since $I(X; Y | Z) \geq 0$, we can conclude that $I(X; Y) \geq I(X; Z)$.
\end{itemize}
\end{theorem}


\paragraph{Sufficient Statistics Connection: } Assume that $X \to Y$ is a Markov
chain. E.g., $X$ are some parameters of an underlying distribution and $Y$ are
some data collected from the distribution. If $Z = f(Y)$, then we have $X \to Y
\to Z$. Therefore any function $Z$ on the data $Y$ cannot have greater
information on the underlying distribution $X$ than $Y$ did in the first place. 

A \textbf{sufficient statistic} is one that has all the information that the
data had about the underlying distribution. That is, $Z = f(Y)$ is a sufficient
statistic on $Y$ if $I(Z; X) = I(Y; X)$. 

\paragraph{Minimal Sufficient Statistic: } Let $\theta \to T(X) \to U(X) \to X$.
$T(X)$ is the \textit{minimal sufficient statistic} iff $U(X)$ can be any
sufficient statistic and still have $\theta \to T(X) \to U(X) \to X$ be a valid
Markov chain. 





\section{Fano's Inequality}

Fano's inequality concerns estimators for random variables. Given some
correlated random variables $X, Y$, we want to understand the probability of
error when using an estimator $\hat X(Y)$ to approximate $X$. The big reveal is
that $\Pr(\text{error}) = \text{function}(H(X|Y))$.


\begin{theorem}{Fano's Inequality}
Let $X,Y$ be dependent random variables. $\hat X(Y)$ is an estimator on $X$, so
$X\to Y\to \hat X$ is a valid Markov chain for the joint distribution. Then 
\begin{align}
	\label{eqn:fano}
	H(P_{err}) + P_{err} \log |\mathcal X| \geq H(X | \hat X) \geq H(X | Y)
\end{align}

A weaker form of the same being: 
\begin{align}
	\label{eqn:fano_weak}
	1 + P_{err} \log |\mathcal X| &\geq H(X | Y) \\
	P_{err} &\geq \frac{H(X| Y)-1}{\log |\mathcal X|} \\
\end{align}

\textbf{Proof Sketch:} 
\begin{itemize}
\item Represent the ``error'' event as random variable $E$. It takes value 1 if
$\hat X \neq X$ and zero if $\hat X = X$.
\item $P_{err} = \mathbb E[E]$.
\item Expand $H(E, X | \hat X) = H(X | \hat X) + H(E | X, \hat X)$, noting that
the last term must be zero.
\item Also note that $H(E | \hat X) \leq H(E)$ (conditioning reduces entropy).
\item $H(X | E, \hat X)$ can be expanded and shown to be bounded by $P_e \log
|\mathcal X|$.
\item Finally use Markov's inequality for $H(X | \hat X) \geq H(X | Y)$. 
\item Combine everything and you should be able to get the strongest version in
Equation~\ref{eqn:fano}.
\end{itemize}
\end{theorem}

You can also strengthen it to $$H(P_{err}) + P_{err} \log (\underbrace{|\mathcal
X| - 1}_{(\star)}) \geq H(X|Y)$$ since one element of $\mathcal X$ is eliminated
from the error entropy calculation by random guessing.


\paragraph{``Sharpness'' of Fano's Inequality: } If you have no information $Y$
to inform $\hat X$, your best guess is $\hat X = \arg\max_x p(x)$. Equality in
Fano's inequality (i.e., $H(P_{err}) + P_{err} \log(|\mathcal X| - 1) \geq
H(X)$) is achieved by $p(\hat x) = 1-P_{err}$ and $p(x \neq \hat x) =
P_{err}/(|\mathcal X|-1)$.


\paragraph{Bound on Collision: } Let $X, X' \sim p(x)$. Then $\Pr\{X = X'\} =
\sum_{x\in \mathcal X}^{} (p(x))^2$. Then \begin{equation}
\Pr(X = X') \geq 2^{-H(X)}
\end{equation}

with equality if $X\sim unif(\mathcal X)$. 

\textbf{Proof sketch:} Expand RHS with entropy in $\mathbb E[]$ form. Apply
Jensen's inequality, and you'll recover $\sum p(x)^2$ as the upper bound. 


\paragraph{Collisions with Different Distributions: } Let $X\sim p(x)$ and
$\hat X \sim r(x)$. Then 
\begin{align}
	\Pr(X\neq \hat X) &\geq 2^{-H(p) - D(p\|r)} \\
	\Pr(X\neq \hat X) &\geq 2^{-H(r) - D(r\|p)} \\
\end{align}
\textbf{Proof sketch:} We know the LHS is $\sum_x p(x)r(x)$. Expanding the RHS,
we can apply Jensen's inequality if $H$ and $D$ are in $\mathbb E$ form. Then we
will have a bound equal to LHS.










\chapter{Asymptotic Equipartition Theorem (AEP)}

\section{Typicality and AEP Theorem}

AEP is the application of the \textbf{weak law of large numbers} (WLLN) to
entropy.  Recall WLLN: 
\begin{equation}
	\label{eqn:wlln}
	\lim_{n\to \infty} \frac{1 }{n} \sum_{i=1}^{n} X_i = \mathbb E[X]
\end{equation}

Applying to entropy, 

\begin{theorem}{Asymptotic Equipartition Theorem}
Let $X_1 \dots X_n$ be iid with PMF $p(x)$. Then 
\begin{align}
	\label{eqn:aep}
	\frac{1}{n} \log \frac{1}{p(X_1, \dots, X_n)} &\to H(x) \\
	p(X_1, \dots, X_n) &\to 2^{-n H(x)}
\end{align}

in probability. That is, for all $\epsilon > 0$, $\exists$ $n$ such that the
difference between the sample entropy (LHS) and the true entropy (RHS) is less
than $\epsilon$.

\textbf{Proof sketch:} Apply WLLN to the definition of entropy. LHS = RHS in
limit $n\to \infty$..
\end{theorem}


\paragraph{Typical sequences: } Sequences of $x_i$ with \textbf{sample entropy}
$\approx nH(X)$. 

\paragraph{Typical set $A_\epsilon^{(n)}$ w.r.t. $p(x)$: } A set of sequences
$x^n\in \mathcal X^n$ that satisfy 
\begin{align}
	\label{eqn:typical_set}
	2^{-n(H(X) + \epsilon)} \leq p(x_1, \dots, x_n) \leq 2^{-n(H(X) - \epsilon)}
\end{align}


\begin{theorem}{Properties of $A_\epsilon^{(n)}$}
\begin{enumerate}
\item $x^n \in A_\epsilon^{(n)}$ $\to$ sample entropy $- \frac 1 n \log p(x^n)
\in [H(X)-\epsilon, H(X)+\epsilon]$.
\item $\Pr\{X^n \in A_\epsilon^{(n)}\} = \Pr\{A_\epsilon^{(n)}\} \geq
1-\epsilon$ for large $n$.
\item $|A_\epsilon^{(n)}| \leq 2^{n(H(x) + \epsilon)}$ for all $n$.
\item $A_\epsilon^{(n)} \geq (1-\epsilon) 2^{n(H(x) - \epsilon)}$ for
sufficiently large $n$.
\end{enumerate}

\textbf{Proof Sketches:} 
\begin{enumerate}
\item Rearrangement of AEP/definition of $A_\epsilon^{(n)}$.
\item Apply the lower bound on $\Pr{x^n}: x^n\in A_\epsilon^{(n)}$.
\item $\Pr\{A_\epsilon^{(n)}\} \leq 1$. But we also know that $\Pr(x^n) \geq
2^{-n(H(x) + \epsilon)}$ if $x^n \in A_\epsilon^{(n)}$. Do the math.
\item $\Pr\{A_\epsilon^{(n)}\} \geq 1-\epsilon$ (from 2). Since typical
sequences $x^n$ have maximum probability $2^{-n(H(X)-\epsilon)}$, we can derive
this bound.
\end{enumerate}

\end{theorem}





\chapter{Data Compression}

\textit{Tail end of EIT chapter 3 and the entirety of chapter 5}.


\paragraph{Data Compression -- Problem Statement: } We want to find the
\textit{shortest} codes for transmitting sequences $x^n$ where each token is iid
with PMF $p(x)$. 
\begin{itemize}
\item We can leverage notions of \textbf{typicality} since we know that $X^n \in
A_\epsilon^{(n)}$ with arbitrarily high probability for large $n$.
\item \textbf{Simple implementation:} Introduce some ordering to elements in
$A_\epsilon^{(n)}$. Since the size of $A_\epsilon^{(n)} \approx 2^{nH(X)}$, a
binary index would need $n(H+\epsilon) + 1$ bits. This is a pretty good code
already!
\item For items not in $A_\epsilon^{(n)}$, we can prepend a flag bit and use
codes of length $n \log |\mathcal X| + 1$.
\end{itemize}




\section{Definitions \& Source Coding Theorem}

\paragraph{Compression/Source Coding Preliminaries: } 
\begin{itemize}
\item \textbf{Source:} Random variable $X: x\in \mathcal X$.
\item \textbf{Codeword Alphabet:} $\mathcal D$ is a $D$-ary alphabet.
\item \textbf{Source Code $C: \mathcal X \to \mathcal D^*$ } maps from the
source alphabet to strings of arbitrary length from the code alphabet. 
\item \textbf{Expected Length $L(C) = \sum_{x\in \mathcal X} p(x)\ell(x)$} where
$\ell(x) = |C(x)|$.
\item \textbf{Assumption: } We tend to represent every $D$-ary alphabet with
natural numbers $\{0,1,\dots, 1-D\}$.
\item \textbf{Non-singular: } Code $C$ is non-singular iff \begin{align}
	\label{eqn:non_sing}
	x_1 \neq x_t \implies C(x_1) \neq C(x_2)
\end{align}
\item \textbf{Extension Code: } $C^*$ is the extension of $C$ -- \begin{align}
C^*(x_1, x_2, \dots) = C(x_1)C(x_2)\dots
\end{align}
I.e., the concatenation of $C(x_i)$.
\item \textbf{Uniquely Decodable: } $C$ is UD if $C^*$ is non-singular.
\item \textbf{Prefix Code: } No codeword $c(x_1)$ is a prefix of another
codeword $c(x_2)$. This also means one can decode without any future information
-- it is an \textbf{instantaneous code}.
\end{itemize}






\begin{theorem}{Source Coding Theorem}
Let $X^n$ be iid with each $X_i \sim p(x)$. Then \textbf{there exists} a code
with lengths satisfying: 
\begin{align}
	\label{eqn:source_coding_theorem}
	\mathbb E\big[ \frac{1}{n} \ell(X^n) \big] \leq H(X) + \epsilon
\end{align}
for $n$ sufficiently large. In other words, we can represent sequences of length
$n$ using $nH(X)$ bits on average!

\textbf{Proof sketch:} 
\begin{itemize}
\item For large $n$, $\Pr\{A_\epsilon^{(n)}\} \to 1$.
\item Then $\mathbb E[\ell(X^n)] = \sum_{x^n\in \mathbb X^n} p(x^n) \ell(x^n)$.
\item Split the sum into $x^n \in A_\epsilon^{(n)}$ and the compliment.
\item Apply codes of length $n(H+\epsilon) + 2$ for the first sum and lengths $n
\log |\mathcal X| + 2$ to the second (1 flag bit, 1 rounding bit).
\item Simplify using $\Pr\{A_\epsilon^{(n)}\}$ and upper bounds on
$A_\epsilon^{(n)}$ size.
\end{itemize}
\end{theorem}





\section{Kraft Inequality}

The Kraft inequality answers the question, ``how short can codes get?''

\begin{theorem}{Kraft Inequality}
Let $C:\mathcal X^n \to \mathcal D^*$ be a \textbf{prefix code}. Let
$\{\ell_i\}_{i=1}^m$ be the codeword lengths for each $x^n \in \mathcal X^n$
(i.e., $m = |\mathcal X^n|$. Then 

\begin{align}
	\label{eqn:kraft}
	\sum_{i=1}^{m} D^{-\ell_i} \leq 1
\end{align}

And conversely: For any $\{\ell_i\}$ satisfying the inequality, \textbf{there
exists a prefix code with those lengths}!

\textbf{Proof sketch:} 
\begin{itemize}
\item Consider a tree structure for all possible $\mathcal D^*$. Each layer adds
one character, etc. 
\item At the deepest layer $\ell$, there are $D^\ell$ leaf nodes.
\item Each non-leaf node on layer $\ell_i$ has $D^{\ell - \ell_i}$ descendants.
\textbf{To maintain prefix quality}, all descendants are eliminated!
\item The number of descendants on the final layer must sum to $D^{\ell_{max}}$. 
\item Manipulate these equations and the inequality will pop out :)
\end{itemize}
\end{theorem}


\begin{theorem}{Extended Kraft Inequality}
Even for an infinite prefix code (i.e., countably infinite codewords), the
lengths $\{\ell_i\}_{i=1}^\infty$ will satisfy 
\begin{equation}
	\sum_{i=1}^{\infty} D^{-\ell_i} \leq 1
\end{equation}

\textbf{Proof sketch:} Apply analogy to floating point numbers/place value.
"Descendants" represent intervals, they must be disjoint, etc. 
\end{theorem}



\section{Finding Optimal Codes}

We learned from the Kraft Inequality (Equation~\ref{eqn:kraft}) that the
codeword lengths are constrained to follow $\sum D^{-\ell_i} \leq 1$. Optimal
codes will therefore solve the following optimization problem:
\begin{align}
\min_{\{\ell_i\}} & \sum_{i=1}^{m} p_i \ell_i \\ 
s.t. & \sum_{i=1}^{m} D^{-\ell_i} \leq 1
\end{align}


\paragraph{Lagrange Multiplier Solution: } $\ell_i^* = -\log_D p_i$ satisfies
Kraft inequality and minimizes $\mathbb E[\ell_i]$ -- expected code length
converges to $H_D(x)$. However, we need to round off code lengths so they're
integers!

\begin{theorem}{Expected code length inequality}
For a $D$-ary code and random variable $X: x\in \mathcal X$, 
\begin{align}
	L \geq H_D(X)
\end{align}
with equality iff $D^{-\ell_i} = p_i$ where $p_i = p(x_i)$.

\textbf{Proof sketch:} Start by expanding $L - H_D(X)$. Combine the sums using
product-sum log rule and recover $L - H_D(X) = D(\mathbf p \| \mathbf r)$ where
$r_i = D^{-\ell_i}/\sum_j D^{-\ell_j}$. By non-negativity of $D(\|)$, the proof
is done.
\end{theorem}



\paragraph{D-adic Distributions: } $p(x)$ is D-adic if $\exists \{n_i\}$ such
that \begin{equation}
p(x_i) = D^{-n_i}
\end{equation}
where each $n_i$ is a natural number.


Generally, we want a $D$-adic distribution $p(x)$ so that our codes achieve
expected length $L = \sum p_i \ell_i = H_D(X)$. Failing that, we want to
\textbf{minimize $D(d-adic \| p(x))$}. We are approximating $p(x)$ with the
closet $D$-adic distribution. That's really all that coding methods boil down
to!
\begin{itemize}
\item Shannon-Fano: Offers good, easy, suboptimal codes. 
\item Huffman: Truly optimal codes based on $p(x)$ -- we actually find the
nearest $D$-adic distribution!
\end{itemize}


To summaryze: we have just established some bounds on \textbf{code lengths
$\{\ell_i\}_{i=1}^m$} for $n$-ary transmissions from the set $\mathcal X^n$, we
can go ahead and take a look at the bounds on \textbf{expected code lengths} $L
= \mathbb E[\ell_i] = \sum_{x^n\in \mathcal X^n}$! This will help us understand
the true practical information transmission limitations -- after all, our
concern is primarily in aggregate behavior for communication systems.


\begin{theorem}{1-Bit Bound on Expected Code Length $L$}
Let $L$ represent the expected code length $L = \mathbb E_{p(x)}[\ell(x)]$. Then
the optimal code will produce the minimum expected code length $L^*$ where $L^*$
is bounded as
\begin{align}
	\label{eq:one_bit_bound}
	H(X) \leq L^* \leq H(X) + 1
\end{align}

\textbf{Proof sketch:} 
\begin{itemize}
\item Recall that the optimal code will have lengths $\{\ell_i\}$ that give rise
to the nearest diadic distribution $p(x_i) \approx D^{\ell_i} / \sum
D^{\ell_i}$. 
\item Let us introduce $\mathbf r$ as follows: 
	$$r_i = \{\frac{D^{-\ell_i}}{\sum_j D^{\ell_j}}\}$$
\item Then we can reframe the search for optimal code lengths $\{\ell_i^*\}$ as
the minimization of the following function: 
	$$\min_{\mathbf \{\ell_i\}} \big[ D(\mathbf p\| \mathbf r)\big] - \log (\sum
	D^{-\ell_i})$$
\end{itemize}

\item Instead of doing anything clever, let's just use the \textbf{rounded
version of the non-integer solution}. That is, 
	\begin{equation}
		\ell_i = \lceil \log_D \frac{1}{p_i} \rceil
	\end{equation}
\end{theorem}


\paragraph{Arbitrary Closeness to Optimal $L = H(X)$: } Observe that we are able
to make the per-character average length $\frac{1}{n} L \to \frac 1 n H(X^n) = nH(X)$
arbitrarily close by increasing $n$ since $H(X^n)+1$ is an upper bound on
optimal $L^*$.


\paragraph{Wrong Code Cost: } If we create optimal codes according to $q(x)$,
then $\ell_i = \lceil \log \frac 1 {q(x_i)} \rceil$. The resulting code length
under real conditions $p(x)$ will be $\mathbb E_p [\ell(X)]$. It is given by 
\begin{equation}
	\label{eqn:wrong_distro}
	H(p) + D(p \| q) \leq \mathbb E_p [\ell(X)] < H(p) + D(p \| q) + 1
\end{equation}

\textbf{Proof sketch:} Start by expanding $\mathbb E_{p(x)} [\ell(X)]$ into sum form,
with $\ell(x) = \lceil \log \frac 1 {q(x)} \rceil$. You'll get a sum-log term
that you can expand into $H(p)$ and $D(p\|q)$ by multiplying the term in the
$\log$ by $p(x)/p(x)$ \qedsymbol{}



\subsection{Generalizing Kraft Inequality to all UD Codes}

So far, we have been proving bounds for expected code length in \textbf{prefix
codes}. We now generalize these findings to all \textbf{uniquely decodable}
codes. The big takeaway is that UD codes cannot out-perform prefix codes with
respect to code length. 

\begin{theorem}{McMillan Theorem -- Any UD Code Satisfies Kraft Inequality}
For any uniquely decodable code $c: \mathcal X^n \to \mathcal D^*$, the code
lengths $\ell_i = |c(x_i)|$ for each $x_i \in \mathcal X$ must satisfy 
\begin{equation}
	\sum D^{-\ell_i} \leq 1
\end{equation}

And conversely: Given any $\{\ell_i\}$ satisfying the Kraft inequality, we can
construct a UD code that has lengths $\{\ell_i\}$.

\textbf{Proof sketch:} Our goal is to show that the code lengths must satisfy
$\sum_{x\in \mathcal X} D^{-\ell(x)} \leq 1$.
\begin{itemize}
\item We start by considering the number of different $D$-ary codewords of
length $n$ the code $c(x)$ can produce.
\item Since there only exist $D^n$ unique $D$-ary sequences of length $n$,
$c^k()$ (the $k$th extension of $c$) can only produce $D^n$ sequences of length
$n$.
\item \textbf{Trick}: We do some cursed manipulations of the following term -- 
\begin{align}
	&= \big(\sum_{x\in \mathcal X} D^{-\ell(x)} \big)^k \\ 
	&= \big(\sum_{x_1 \in \mathcal X} D^{-\ell(x_1)} \big) \big(\sum_{x_2 \in
	\mathcal X} D^{-\ell(x_2)} \big) \dots \big(\sum_{x_k \in \mathcal X}
	D^{-\ell(x_k)} \big) \\
	&= \sum_{x_1:k \in \mathcal X^k} D^{-\ell(x_1)}D^{-\ell(x_2)}D^{-\ell(x_k)}\\
\end{align}
The reason this is a valid manipulation is that you can push around $\sum_{x_i}$
around in the product as long as $x_i$ remains within its argument field. 

\item We now apply a change of variables. Specifically, for each sum $\sum_{x^k}
D^{-\ell (x_k)}$, we replace it with $$\sum_{m=1}^{k \ell_{max}} a(m) D^{-m}$$ where
$a(m)$ is the number of $m$-long codes. This is equivalent to $\sum_x
D^{-\ell(x)}$ -- we are just grouping and calculating by code lengths $m$. 

\item $a(m)$ is the number of $m$-long codes, and there cannot be more than
$D^m$ of those. So all of a sudden, we have 
	\begin{align}
		&= (\sum_{x\in \mathcal X} D^{-\ell(x)})^k \\ 
		\sum_{x^k \in \mathcal x^k} D^{-\ell(x^k)} &= \sum_{m=1}^{k \ell_{max}}
		a(m) D^{-m} \\ 
		& \leq \sum_{m=1}^{k \ell_{max}} D^m D^{-m} \\ 
		&= \ell_{max} k
	\end{align}

\item So now we know that 
	\begin{align}
		(\sum_{x\in\mathcal X} D^{-\ell(x)})^k &\leq \ell_{max} k \\ 
		\implies \sum_{x\in \mathcal X} D^{\ell(x)} &\leq (\ell_{max} k)^{1/k} \\ 
		\implies \sum_{x \in \mathcal X} D^{-\ell(x)} &\leq 1
	\end{align}
	With the last transformation justified by the fact that $\lim_{k\to\infty}
	\ell_{max} k = 1$

\item Therefore Kraft's inequality holds not only for prefix codes, but for all
UD codes \qedsymbol{}.
\end{itemize}




\end{theorem}






\section{Huffman Codes}

Huffman coding is provably optimal -- that is, it yields the minimum possible
expected code length $L = \mathbb E_{p(x)}[\ell(x)]$. The general idea is to
continually generate a \textbf{shared prefix} for the $D$ least likely remaining
symbols. Once grouped, the codes for each of the least likely symbols will only
differ in the last code character. The weird \textbf{recursive} part of the
algorithm is how the new \textbf{shared prefix} is treated like a single source
symbol in the next iteration -- hence why the algorithm is so popular as a
dynamic programming/recursion exercise. 

Overall, the intuition boils down to this: compression is aided by
\textbf{degeneracy} -- when very similar codes can represent many different
objects. This is an exploitation in the structure of the data: every time we
group together the least likely terms, we are \textbf{adding 1} to their code
length. The other symbols that were untouched essentially get to survive another
iteration without having a character added to their code. In many ways, this is
like the opposite of PCA, matching pursuit, and other compression techniques
that first generate representations of information contained in the \textbf{most
common} elements. One can also connect the ideas behind Huffman coding to
concepts in \textbf{search} -- unlike most search algorithms where the
performance is averaged evenly across search time to all elements, Huffman codes
incorporate \textbf{weights} (i.e., probability of a source token) into the
search procedure. You can even have the weights violate the ``sum to 1'' rule of
probability distributions. Best of all, it's still a provably optimal scheme
with respect to code length/number of search queries!

Overall, Huffman codes are pretty neat :) 

\paragraph{Algorithm Sketch:} 
\begin{enumerate}
	\item Construct a table of source symbols $x\in \mathcal X$ and their
	probabilities $p(x)$. Order this table from high to low probability.
	\item For $D \geq 3$, add dummy symbols with $0$ probability such that the
	number of symbols is $1 + k(D-1)$ for integer $k$.
	\item \textbf{For each iteration:} Combine the least likely $D$ symbols into one
	symbol for the next iteration.
	\begin{enumerate}
		\item Draw arrows from each of the combined symbols to a new entry in a
		new probability column. Assign each arrow a number from $\{0,\dots,D\}$.
		\item Repeat this process with the new probability column. 
	\end{enumerate}
	\item The result of this iterative procedure will be a tree-like structure
	with each path ending at a single symbol in the final column with
	probability $1$.
	\item To construct the codes for each source symbol: Follow the arrows back
	from the $\Pr = 1$ top-level node. 
\end{enumerate}




\subsection{Optimality of Huffman Codes}

Like with most recursive algorithms, the proof for correctness/optimality is
inductive. The central idea to keep in mind is our \textbf{optimality
condition}: we with to solve $\min \sum p_i \ell_i$. 


\paragraph{Big Picture: } To prove the optimality of Huffman codes, we will do the following: 
\begin{enumerate}
\item \textbf{Canonical codes}: Optimal codes can be hard to reason about since there are lots of
different optimal codes. Some will be prefix codes, others won't. Therefore, the
first thing we do is \textbf{narrow the search} for optimal codes to a certain
class. To do this, we need to prove the existence of optimal codes of this
class. We call these codes ``\textbf{canonical codes}'', and their properties
will enable the rest of the proof.
\item \textbf{Induction}: Now that we have established what a ``canonical code''
is, we get to the fun part! We start by naming the ``combine the $D$ least
likely symbols'' operation from the Huffman code generation process a
``\textbf{Huffman reduction}''. We then show that, assuming that the code of the
\textbf{reduced} set of symbols is optimal/canonical, then so will the
\textbf{unreduced} code.
\end{enumerate}


So, let's get started! First, we define a \textbf{canonical code}.
\begin{theorem}{Existence of Canonical Codes}
For all $p(x)$, there exists an \textbf{optimal source code} with the following
properties: 
\begin{enumerate}
\item $p(x_1) > p(x_2) \implies \ell(x_1) \leq \ell(x_2)$.
\item The two longest $\ell(x_i), \ell(x_j)$ are equal. 
\item The two longest $c(x_i), c(x_j)$ differ \textbf{only in the final bit}.
They also correspond to the \textbf{least likely source symbols}.
\end{enumerate}

We call optimal codes with these properties \textbf{canonical codes} (it sounds
awfully clever).

\textbf{Proof sketch:} 
\begin{itemize}
\item How to show $p(x_1) > p(x_2) \implies \ell(x_1) \leq \ell(x_2)$: 
	\begin{enumerate}
	\item Imagine swapping the codewords for $x_1, x_2$. 
	\item Then the sum $\sum p_i \ell_i$ will be strictly greater than if they
	were unswapped \qedsymbol{}
	\end{enumerate}

\item How to show that the two longest $\ell(x_i), \ell(x_j)$ will have the same
lengths: 
	\begin{enumerate}
	\item \texttt{Assume toward a contradiction} that they have different
	lengths. 
	\item Recall the prefix property: the second longest cannot be a prefix of
	the longest. 
	\item Therefore deleting the last character(s) of the longest would yield a
	shorter code that is still unique.
	\item This would strictly reduce $\mathbb E[\ell(x)]$! \qedsymbol{}
	\end{enumerate}

\item How to show that the two longest codes only differ in the final bit AND
correspond to the lowest probability symbols: 
	\begin{enumerate}
	\item For an optimal prefix code, imagine that the longest two codes are NOT
	siblings. That is, they differ in more than just the final bit. 
	\item Then we should be able to delete the last bit of each of them and
	still have a UD/prefix code (since none of the other codes will be prefixes
	of them). 
	\item We can also just make them siblings with no cost to code lengths.
	\item Therefore, the longest two codes should only differ in the last bit.
	They should also correspond to the lowest probability $x\in \mathcal X$
	given property (1) \qedsymbol{}
	\end{enumerate}
\end{itemize}
\end{theorem}


Now that we've established that canonical codes exist and are optimal, we can
move on and prove that Huffman codes are canonical at each iteration and are
therefore optimal!

\paragraph{Huffman Reduction: } We define a Huffman reduction on a probability
distribution $p(x)$ where $x\in \mathcal X = \{x_1, \dots, x_m\}$. Let $\mathbf
p = [p(x_1), \dots, p(x_m)]^\top$ be ordered from most likely $p(x_1)$ to least
likely $p(x_m)$. Then the Huffman reduction of $\mathbf p$ is 
\begin{equation}
	\mathbf p' = \big[ p_1, p_2, \dots, p_{m-2}, \underbrace{p_{m-1} +
	p_m}_{\text{reduction!}} \big]^\top
\end{equation}

Now all we have to do is show that the \textbf{optimal code} for the reduced
$\mathbf p'$ can be expanded to the optimal code for unreduced $\mathbf p$.


\begin{theorem}{Optimality of Huffman Codes}
Let $C^*$ be a Huffman code and $C'$ be any uniquely decodable code. Then 
\begin{equation}
	L(C^*) \leq L(C')
\end{equation}

\textbf{Proof sketch:} 
\begin{enumerate}
\item Huffman code generation is essentially a series of \textbf{expansion
operations} from a reduced $\mathbf p'$ to an unreduced $\mathbf p$
\item Use \textbf{proof by contradiction} to show that codes extended from
$\mathbf p' \to \mathbf p$ maintain optimality. 
\item Therefore, if the first code is optimal (must be since it's 1 element),
then the rest of the expansions must also be optimal. Therefore, Huffman coding
produces optimal codes \qedsymbol{}
\end{enumerate}
\end{theorem}





\section{Shannon Codes}

Shannon codes give a procedure to construct a prefix code with lengths $\ell(x)
= \lceil \log \frac 1 {p(x)} \rceil$. These aren't optimal like Huffman codes,
but they're pretty good. In fact, there's a whole section in the textbook
dedicated to its ``competitive optimality'' with Huffman codes. Perhaps people
feel the need to defend Shannon coding because Shannon is so central to
information theory.  Regardless of whether it is deserved, you'll probably be
asked to do a Shannon code example at some point in your life, and it's a
somewhat elegant mathematical construction. 

\paragraph{Big Picture: } I'm not sure if this is how they were developed, but
this is the story I tell myself.
\begin{enumerate}
\item We know that $\ell(x) = \lceil \log \frac 1 {p(x)} \rceil$ is achievable.
But if someone asks me to actually code some $x$, where do I even get the values
for $c(x)$? Do I make them randomly? 
\item \textbf{CDF encoding idea:} The CDF of a random variable has a really nice
property: let $F(x)$ be the CDF of random variable $X$. Then $F:\mathcal X \to
[0,1]$ in a \underline{one-to-one} manner! So let's try using the bits of
$F(x)$ to create our code $c(x)$!
\item That's great, but $F(x)$ could take an infinitey number of bits to encode!
We need to \textbf{truncate} them somehow... 
\item \textbf{Truncation technique:} Just take the first $\ell(x) = \lceil\log
\frac 1 {p(x)} \rceil + 1$ bits from $F(x)$ to get truncated $\lfloor
F(x)\rfloor_{\ell(x)}$.
\item We can show that this will still produce a prefix code via \textbf{extreme
cleverness}.
\end{enumerate}


I'll elaborate more on that last point (why the truncation technique ends up
working) momentarily, but that's pretty much how Shannon coding works. The only
important caveat is that we don't use $F(x)$ directly -- we use bits from a
modified CDF $\tilde F(x) = F(x) + \frac 1 2 p(x)$. This corresponds to the
midpoint between $F(x-1)$ and $F(x)$.

\paragraph{Big result: } The scheme gives us a code with expected length $L \leq
H(X) + 2$. Aren't we so lucky for that.


\paragraph{Why does $\lceil \tilde F(x) \rceil_{\ell(x)}$ work? } What a great
question! Let's start by considering what ``prefix-free'' corresponds to in the
context of using bits from the binary representation of $\tilde F(x)$. 
\begin{itemize}
\item Let's assume you truncate to digit $\ell$ after the decimal point. Under
the prefix condition, no codes that share those first $\ell$ digits. 
\item This corresponds to \textbf{claiming} the range $[0.z_1z_2\dots z_\ell,
0.z_1z_2\dots z_\ell + \frac 1 {2^\ell}]$. That last $+\frac 1 {2^\ell}$ term
corresponds to the extra value if all the digits past $z_\ell$ were ones. 
\item Now you just have to show that there's no \textbf{overlap} in the
\textbf{claimed regions} of each $x$!
\item At that point, it's just a bunch of symbol shunting. So I really can't be
bothered to write it all out, and I suggest you refrain as well -- I suspect
that symbol shunting is bad for one's health.
\end{itemize}


That's pretty much Shannon coding. You get some decent preformance out of it,
it uses some clever ideas, and sometimes thats enough. I'm not going to get into
the ``competitive optimality'' too much since it's terribly boring, but these
are the main takeaways:
\begin{enumerate}
\item If $ell(x)$ are Shannon code lengths and $\ell'(x)$ are any other UD code,
then $\Pr(\ell(x) \geq \ell'(x) + c) < \frac 1 {2^{c-1}}$.
\item Given a dyadic $p(x)$ (expressible in base 2), let $\ell(x) = -\log p(x)$
and $\ell'(x)$ be any other Shannon code. Then $\Pr (\ell(X) < \ell'(x)) \geq
\Pr(\ell(X) > \ell'(x))$. 
\end{enumerate}
See? I told you it was boring. 



\paragraph{An important note on ``competitive optimality'': } Most of this stuff
is framed in probabilities that one is shorter/longer than the other. The reason
Shannon coding secretly sucks is that it's pretty suboptimal in practical
\textbf{expected length}. If you have a distribution $p(x_1) = 0.001, p(x_2) =
0.999$ then Shannon coding would tell you to use insane large code lengths for
$x_1$ for no reason. The point is, don't believe all the propaganda about
Shannon coding. 


\chapter{Channel Capacity}

We've spent a lot of time on what essentially amounts to data compression. If
you ever make a friend, though, you may actually need to \textit{send} data. So
let's study ``channels'' -- stuff you would send that information through.

\paragraph{Discrete Channel Definition: } 
\begin{enumerate}
\item Input alphabet $\mathcal X$. 
\item Output alphabet $\mathcal Y$.
\item Transition matrix $p(y | x)$ -- this is the actual channel. 
\end{enumerate}

It should be noted that we mainly care about \textbf{memoryless} channels where
the input at a given time depends only on the current input symbol. 


\section{Channel Capacity} 

\paragraph{Channel Capacity: } The amount of information you can transmit on
average via a single use of the channel with vanishingly small error
probability.
\begin{equation}
	C = \max_{p(x)}\{I(X; Y)\}
\end{equation}

Where $p(y | x)$ is dictated by the channel itself. We basically get to ``pick''
out $p(x)$ to maximize $C$. We generally want to get as close to the theoretical
channel capacity as possible. But before we do anything so practical, we need to
faff around with channel capacity calculations and properties of channel
capacity.

\paragraph{Top 10 Tips for Calculating Channel Capacity: } Probably not actually
10, but you get the point.
\begin{itemize}
\item \textbf{Start with the definition.} If no inspiration strikes, just write
down the definition. It's particularly important to write the equivalent forms
of $I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$. 
\item \textbf{Worst case: leverage convexity and solve for $0$ gradient.} Recall
how $I(X; Y)$ is convex w.r.t. $p(x)$ given fixed $p(y|x)$. Representing $p(x)$
as a vector of values, you can solve for the point with zero gradient of $I(X;
Y)$ with respect to each of the values in $p(x)$.
\item \textbf{One-to-many but disjoint $Y$} yields capacity equal to the number
of output clusters. 
\item \textbf{(Maximum) number of bits transmitted} per use of the channel is
identical to capacity. Sometimes this is enough for a hand-wavey calculation of
capacity if you already know what it is. 
\item \textbf{Noisy typewriter} is a big example to keep in mind. If the output
is basically a ``diffused'' version of the input, a uniform distribution will
suffice to maximize. This is basically the same as a symmetric channel.
\item \textbf{Symmetry is your friend.} Check if the channel is symmetric first
-- it gives you great guarantees (e.g., uniform distribution maximizes
capacity).
\item \textbf{Binary erasure channel} with erasure probability $\alpha$ has
capacity $1-\alpha$ if there are two elements in the input space and 3 elements
in the output space (i.e., two correspond to the input and 1 is the ``erased''
outcome).
\end{itemize}



\paragraph{Top 10 Properties of Channel Capacity: } 
\begin{enumerate}
\item $C \geq 0$
\item $C \leq \min \{\log \mathcal X, \log \mathcal Y\}$.
\item $I(X; Y)$ is a continuous function of $p(x)$ and is a \textbf{concave}
function w.r.t. $p(x)$. Therefore local optima are global optima.
\item There's no closed form solution for capacity in the general case. 
\end{enumerate}













\section{Symmetric Channels}

Symmetric channels are great. Can't get enough of them. Nobody loves symmetric
channels more than I do.

\paragraph{Symmetric Channel Definition: } When the rows of the channel are all
permutations of eachother, and so are all the columns. Note that the $x$th row
of the matrix corresponds to the input $x$ and the $y$th column corresponds to
the probability of that output. Essentially, the \textbf{rows sum to 1}.

\paragraph{Properties of Symmetric Channels: } 
\begin{itemize}
\item The maximizing $p(x)$ for $I(X; Y)$ is the uniform distribution.
\item $I(X; Y) \leq \log |\mathcal Y| - H(\mathbf r)$ where $\mathbf r$ is a row
of the transition matrix ($p(:|x)$). 
\end{itemize}

\paragraph{Weakly Symmetric Channels: } Rows are all permutations of eachother
and all columns sum to the same value. Capacity is still $C = \log |\mathcal Y|
- H(\mathbf r)$ and is still achieved by a uniform distribution on the input.





\section{Channel Coding Theorem}

\begin{theorem}{Channel Coding Theorem}
For a discrete memoryless channel, all rates $R < C$ are achievable.
\end{theorem}

Yup, that's the big takeaway. We need to go through a lot to prove it, though.
We start with defining some error probability metrics and what exactly
``achievable'' means. To keep things classy and WLLN-ish, we do most of this in
the $n\to \infty$ regime. That lets us invoke AEP theorem and assume that pretty
much every input/output is \textbf{jointly typical} (which we will also need to
define -- it's not that bad though). After all that, we're ready to prove the
channel coding theorem. We do this by randomly generating our codewords, then
focusing on just the first one $c(x_1)$. Since everything's random, $c(x_1)$ can
stand in for every codeword when we calculate the probability of error. The
receiver just uses \textbf{joint typicality decoding} to determine which output
input sequence $x^n$ matches the received $y^n$. Thanks to AEP stuff, error goes
to zero and everyone's happy.

So let's get cracking with all the definitions and theorems! Lots to do.






\subsection{Definitions}

\paragraph{Discrete memoryless channel: } Defined above, represented as
$\mathcal X, p(y | x), \mathcal Y)$.

\paragraph{$n$th extension of DMC: } Since there's no memory, $(\mathcal X^n,
p(y^n | x^n), \mathcal Y^n$ is well defined. Just let $p(y^n | x^n) =
\prod_{i=1}^n p(y_i | x_i)$.

\paragraph{$(M, n)$ code: } Given a set of $M$ messages $\{1, \dots, M\}$, the
$(M, n)$ code gives an encoding function $X^n: \{1, \dots, M\} \to \mathcal
X^n$. It essentially uses $n$-long sequences from alphabet $\mathcal X$ to
encode one of $M$ messages. It also gives a decoding function $g: \mathcal Y^n
\to \{1, \dots, M\}$. It's a more general scheme than it may seem since the $M$
messages can apply to any collection of $M$ objects -- maybe even source codes. 

\paragraph{Conditional probability of error: } Given that our input message is
$i \in \{1, \dots, M\}$, the conditional probability of error $\lambda_i$ is 
\begin{align}
	\lambda_i &= \Pr \{g(Y^n) \neq i | X^n = x^n(i)\} \\ 
	&= \sum_{y^n \in \mathcal Y^n} p(y^n | x^n(i)) \mathbb I [g(y^n) \neq i] \\
\end{align}

\paragraph{Maximum probability of error: } $\lambda^{(n)}$ is exactly what it
sounds like: 
\begin{equation}
	\lambda^{(n)} = \max_{i\in [M]} \lambda_i
\end{equation}

\paragraph{Arithmetic average error probability: } Also exactly what it sounds
like. 
\begin{equation}
	P_e^{(n)} = \frac{1}{M} \sum_{i=1}^{M} \lambda_i
\end{equation}
It's the general error probability if input messages are chosen uniformly from
$[M]$.


\paragraph{Rate of $(M, n)$ code: } Number of bits per transmission
\begin{equation}
R = \frac{\log M}{n} 
\end{equation}

Though not all rates are \textit{achievable}...

\paragraph{Achievability: } A rate $R$ is achievable if $\exists$ a
\textbf{sequence} of $(\lceil 2^{nR}\rceil, n)$ codes such that 
\begin{equation}
	\lim_{n\to \infty} \lambda^{(n)} = 0
\end{equation}

\paragraph{Capacity: } The \textbf{supremum} of all \textbf{achievable} rates.
Importantly, this implies that a rate less than the capacity implies that an
error probability approaching zero with large block sizes is possible!





\subsection{Joint Typicality}

\begin{theorem}{Jointly Typical Sets Definition}
$A_\epsilon^{(n)} = \{(x^n, y^n)\}$ are the set of $\{(x^n, y^n)\}$ that have
\textbf{sample entropies} that are $\epsilon$-close to the true entropies
(calculated by $p(x^n,y^n)$). 

\begin{align}
A_\epsilon^{(n)} = \{ & (x^n, y^n) \in \mathcal X^n \times \mathcal Y^n: \\ 
& \big| \frac{-1}{n} \log p(x^n) - H(X) \big| \leq \epsilon, \\ 
& \big| \frac{-1}{n} \log p(y^n) - H(Y) \big| \leq \epsilon, \\ 
& \big| \frac{-1}{n} \log p(x^n, y^n) - H(X,Y) \big| \leq \epsilon, \\ 
\}&
\end{align}
\end{theorem}


\begin{theorem}{Joint AEP}
Let $(X^n, Y^n)$ be sequences generated with with distribution $p(x^n, y^n) =
\prod_{i=1}^n p(x_i, y_i)$ (iid). Then 

\begin{align}
\Pr\{ (X^n, Y^n) \in A_\epsilon^{(n)} \} &\to 1 \text{ as } n\to \infty \\
|A_\epsilon^{(n)}| & \leq 2^{n(H(X, Y) + \epsilon)} \\ 
\Pr\{ (\tilde X^n, \tilde Y^n) \in A_\epsilon^{(n)} \} & \leq 2^{-n(I(X; Y) - 3\epsilon)} \\
\Pr\{ (\tilde X^n, \tilde Y^n) \in A_\epsilon^{(n)} \} & \geq (1-\epsilon)2^{-n(I(X; Y) + 3\epsilon)} 
\end{align}

Where $(\tilde X^n, \tilde Y^n) \sim p(x^n)p(y^n)$ are \textbf{independently
sampled} sequences.
\end{theorem}

The big upside are those last two statements -- as long as you increase $n$, the
probability of incorrectly associating two $\tilde X^n, \tilde Y^n$ that aren't
actually sampled from $p(x,y)$ is vanishingly small as long as you use jointly
typical decoding. 

\paragraph{Proof sketch: Probability of $A_\epsilon^{(n)} \to 1$ } Literally
just use the law of large numbers. Sample entropy convergest to expected value
which is the actual entropy for all 3 of the conditions in the definition of
$A_\epsilon^{(n)}$.

\paragraph{Proof sketch: $|A_\epsilon^{(n)}| \leq 2^{n(H(X, Y) + \epsilon)}$ }
The definition states the max/min probability of the sequence $(x^n, y^n)$ is
it's in $A_\epsilon^{(n)}$ -- you just need to rephrase the entropy constraint.
Using this, and the fact that the probability of the typical set in total is
less than 1, you get this bound.

\paragraph{Proof sketch: Independently sampled sequences have low probability in
typical set. } This proof is actually a little cursed. I'm just going to have to
write the 3 cursed lines:
\begin{align}
	\Pr \big( (\tilde X, \tilde Y) \in A_\epsilon^{(n)} \big) &= \sum_{(x^n,
	y^n) \in A_\epsilon^{(n)}} p(x^n)p(y^n) \\ 
	&\leq \underbrace{ 2^{n(H(X, Y) + \epsilon)} }_{|A_\epsilon^{(n)}|} 
		\cdot \underbrace {
			2^{-n( H(X) - \epsilon )} \cdot 2^{-n( H(Y) - \epsilon )}
		}_{\text{Max } p(\tilde x^n), p(\tilde y^n): (\tilde x^n, \tilde y^n) \in A_\epsilon^{(n)} }
		\\ 
	&= 2^{-n (I(X; Y) - 3\epsilon)}
\end{align}






\subsection{Proof of Channel Coding Theorem}

Finally we can get to the channel coding theorem proof! After all the work we
have put in, it's not so bad -- the starting point is easy to forget, though.



\paragraph{Messaging Procedure for Channel Coding Theorem: } Just to make it
completely clear, this is the procedure we use to transmit and receive messages
under the channel coding theorem:
\begin{enumerate}
\item Sender selects some message $W\in [M]$ from the index set to send. We
generally assume that this is uniformly sampled. 
\item Sender encodes $i$ as $x^n(W) \in \mathcal X^n$ -- the codeword for index
set message $W$.
	\begin{itemize}
	\item The ``codebook'' $C$ consists of all $x^n(w): \; w\in [M]$. 
	\end{itemize}

\item Sender chucks $x^n(i)$ into the channel $p(y^n| x^n)$ and the receiver
samples $y^n$ from that channel's probabilistic output.
\item The receiver \textbf{guesses} the original message $W$ using the following
estimation procedure: We find $\hat W$ such that
	\begin{itemize}
	\item Find $\hat W: (x^n(\hat W), y^n) \in A_\epsilon^{(n)}$ -- i.e.,
	jointly typical decoding.
	\item \textbf{Non-Unique Case} If $\nexists W' \neq \hat W$ such that
	$(x^n(W'), y^n) \in A_\epsilon^{(n)}$ AND $(x^n(\hat W), y^n) \in
	A_\epsilon^{(n)}$, \textbf{RETURN ERROR}.
	\end{itemize}
\end{enumerate}

Our goal is essentially to understand the bounds on error probabilities
associated with this procedure. Errors occur when we return \textbf{ERROR} and
also when we incorrectly decode $\hat W$ -- i.e., $\hat W \neq W$.




\paragraph{Starting the Channel Coding Theorem Proof: } Our goal is to show that
all rates of transmission $R < C$ are \textbf{achievable} -- that is, we can
construct a sequence of $(M, n)$ codes with that rate $R$ such that the
\textbf{maximum error probability} $\lambda^{(n)}$ goes to zero as $n$ goes to
infinity.

That's all a bit of a mouthful, but you should read it until it's seared into
your memory. We start the channel coding theorem by replacing $M$ with $2^{nR}$
in the $(2^{nR}, n)$ code statement. Recall that rate $R = \log(M) / n$. So for
any given $n, R$, setting $M := 2^{nR}$ will guarantee that rate. From there,
you just have to prove \textbf{achievability} -- that $\lambda^{(n)} \to 0$.

So, to summarize our roadmap: 
\begin{enumerate}
\item We want to show that rates $R < C$ are achievable. 
\item Achievability of $R$ $\equiv$ $\exists$ a sequence of $(M, n)$ codes with
rate $R$ such that max error probability $\lambda^{(n)} \to 0$ as $n\to \infty$.
\item We set $M := 2^{nR}$. This yields a code with rate $R = \log (M)/n =
\log(2^nR)/n = R$. 
\item To show \textbf{achievability}, we now just need to show that
$\lambda^{(n)} \to 0$ for some sequence of codes!
\end{enumerate}


\paragraph{Random Codes: Big Idea 1 } As everyone likes to harp on about,
Shannon was a very clever guy. One of the clever things he did was use
\textbf{random codes} to prove the channel coding theorem. For each of the $M =
2^{nR}$ input messages, he generated a random $x^n(i):\; i\in [M]$ where each
index $x_j\sim p(x)$ for some fixed $p(x)$. We then show that $\lambda^{(n)} \to
0$ under the condition $R < C$ when we use \textbf{joint typicality decoding}.
Hopefully this makes you glad we did all that stuff with joint AEP, because we
basically just get to yeet that in at the end to prove that the error rates go
to zero. 


\paragraph{Bound Error Rate for $x^n(1)$: Big Idea 2 } Another life hack we get
to use in the proof is to use $x^n(1)$ (i.e., the codeword for input number 1
from index set $[M] = 2^{nR}$) as a stand-in for all $x^n(i)$ when bounding
error probabilities. This is allowable since all the $x^n(i)$ were generated
randomly using the same process. This really improves our quality of life over
the course of the proof. 



\paragraph{Channel Coding Theorem Proof Sketch: } 
\begin{itemize}
\item Message is sent: $W \to x^n(W) \to y^n$ where each element of $x^n(W)$ is
iid with $p(x)$.
\item Sender and receiver both have access to codebook $C = \{x^n(w)\}_{w=1}^M$
as well as $p(y|x), p(y^n | x^n)$. They are therefore both able to construct and
search $A_\epsilon^{(n)}$
\item \textbf{Errors} occur when $\hat W \neq W$ and when $\exists W' = \hat W$ that is
also yields jointly typical $x^n(W'), y^n$.
\item Let $x^n(1)$ stand in for all $x^n(w):\; w\in [M]$ as above.
\item Let $\mathcal E$ be the ``error'' event $\{\hat W(y^n) \neq
W\}$ OR the decoding procedure produces an error. Then 
	\begin{equation}
		\Pr(\mathcal E) = \Pr(\mathcal E | W = 1)
	\end{equation}

\item Let $E_i$ be the event of decoding $\hat W = i$ when $W = 1$. That is, 
	\begin{equation}
		E_i = \{(X^n(i), Y^n) \in A_\epsilon^{(n)}\}
	\end{equation}
	
\item $\mathcal E$ can be written as a union of individual outcomes decoding $E_i$ as
follows: 
	\begin{align}
		\Pr(\mathcal E | W = 1) &= \Pr \big( \neg E_1 \cup E_2 \cup E_3 \cup
		\dots \cup E_{2^{nr}} | W = 1 \big) \\
		&\leq \Pr(\neg E_1 | W = 1) + \sum_{i=2}^{2^{nR}} P(E_i | W = 1)
	\end{align}

\item Now we get to use the AEP outcomes on each of those terms!
	\begin{itemize}
	\item $Pr(\neg E_1 | W_1) \to 0$ as $n\to \infty$ by joint AEP. All sequences
	$(x^n, y^n)$ sampled iid from $p(x, y) = p(y | x) p(x)$ will end up in
	$A_\epsilon^{(n)}$ if $n$ is increased enough.
	\item We use the \textbf{independence statement} in the joint AEP to bound
	the rest of the terms $\Pr(E_{i\neq 1} | W_1)$. For $i \neq 1$, $x^n(i)$ is
	independent from $x^n(1)$ since the code generation process uses iid
	sampling of $p(x)$ to generate each code. Therefore, we can view the pair of
	``mismatch'' sequences $x^n(i\neq 1), y^n$ as \textbf{independently sampled}
	from $p(x), p(y)$ respectively.
	\item Joint AEP states that the probability of two \textbf{independently
	sampled} $(\tilde x^n, \tilde y^n) \in A_\epsilon^{(n)}$ is 
		\begin{equation}
			\Pr\{(\tilde X^n, \tilde Y^n) \in A_\epsilon^{(n)}\} 
			\leq 
			2^{-n(I(X; Y) - 3\epsilon)}
		\end{equation}
	\end{itemize}

\item \textbf{Home stretch!} 
	\begin{align}
		\Pr(\mathcal E) &= \Pr(\mathcal E | W=1) \\ 
		& \leq \Pr(\neg E_1 | W = 1) + \sum_{i=2}^{2^{nR}} P(E_i | W = 1) \\
		& \leq \epsilon + (2^{nR} - 1) (2^{-n(I(X; Y) - 3\epsilon)}) \\
		& \leq \epsilon + (2^{3n\epsilon}) (2^{-n(I(X; Y) - R)}) 
	\end{align}

	And so, as long as the second exponential argument $I(X; Y) - R > 0$, the
	whole thing will vanish to zero as $n\to \infty$. Thus we have proven that
	all rates $R < I(X; Y)$ are \textbf{achievable} (i.e., vanishingly small
	error probability as block length is increased)! \qedsymbol{}
\end{itemize}


That's the meat of the proof! All that's left is some fairly elementary
strengthening of a few of the statements. 

\paragraph{Strengthening the Channel Coding Theorem: } 
\begin{itemize}
\item Use the capacity-maximizing $p^*(x)$ so that the condition $R < I(X; Y)
\to R< C$.

\item Search for the \textbf{optimal codebook} to get average error probability
less than $2\epsilon$.
	\begin{equation}
		C^* = \arg\min_C \Pr(\mathcal E | C)
	\end{equation}

\item \textbf{Markov Inequality Trick:} Let's throw away the worst half of
$C^*$. This gives us a bound on the \textbf{maximum value} of $\lambda_i(C^*)$
rather than just guarantees on average/expected value. Half of the $\lambda_i$
values \textbf{must} have values all less than $4\epsilon$ if the average
overall is $2\epsilon$. This results in $2^{nR-1}$ remaining codewords with new
rate $R' = R - \frac 1 n$, but we get in return $\max \lambda_i = 4\epsilon$!
\end{itemize}

That's pretty much it! 


\section{Converse to Channel Coding Theorem}
\textit{Todo} 

\section{Hamming Codes}
\textit{Todo} 

\section{Feedback Codes} 
\textit{Todo} 










\chapter{Continuous Stuff}

If you've been outside or touched grass at any point in your life, you may have
noticed that many things are actually continuous and not discrete. Unfortunately
for you, we've spend most of the course looking at discrete distributions and
codes, so now we need to make great haste to cover our ass and make sure this
stuff works for continuous random variables. 


\section{Differential Entropy}

Differential entropy is defined for \textbf{continuous random variables} with
CDF $F(x)$ and PDF $f(x)$. A term that often comes up is the ``support set'' of
$f(x)$ -- it's just $S = \{x\in \mathcal X : \; f(x) > 0\}$. Because god forbid you
just say "regions where $f(x)>0$...

It's also good to keep in mind that the existence of integrals and PDF's can't
be taken for granted. Everything is qualified with ``if it exists''. 


\paragraph{Differential Entropy Definition: } 
\begin{align}
	h(X) &= - \int_{S} f(x) \log f(x) dx \\ 
		 &= - \mathbb E_{f(x)} [\log f(x)] 
\end{align}

Concerningly, you can have negative differential entropies. You can interpret
$n+h(X)$ as the \textbf{number of bits} required to get $n$-bit accuracy on $X$.
The reason you can have negative $h(X)$ is that sometimes you can get $n$-bit
accuracy with less than $n$ bits -- e.g., if you know the RV $X$ will
\textbf{always} have 3 leading zeros, you get 3 bits of accuracy for ``free''. 


\subsection{Discrete $\to$ Differential Entropy}

We can connect discrete entropy to differential entropy much like how we make
most continuity arguments: we quantize differential entropy into progressively
finer discrete Riemann-like approximations of the PDF $f(x)$, then we
congratulate ourselves when it converges to our definition of differential
entropy.

\paragraph{Quantization Definition: } For any continuous random variable $X$, we
define the quantization $X^\Delta$ with ``bin length'' $\Delta$ as 
\begin{align}
	X^\Delta = x_i &\text{ if } i \Delta \leq X \leq (i+1)\Delta \\
	\iff \Pr(X^\Delta = x_i) &= f(x_i) \Delta 
\end{align}


\paragraph{Convergence of Quantized Entropy $\to$ Differential Entropy: }
Assuming that $f(x)\log f(x)$ is \textbf{Riemann integrable}, we have a pretty
straight-forward convergence of $H(X^\Delta) + \log \Delta \to h(X)$ as $\Delta \to 0$.
\begin{equation}
	H(X^\Delta) + \log \Delta \to h(X) \text{  as } \Delta \to 0
\end{equation}

In other words, the entropy of an $n$-bit quantization of continuous variable
$X$ (i.e., $-\log \Delta$-bit) has discrete entropy approximately equal to $h(X)
+ n$. The number of partitions is roughly $1/\Delta$ which would require $\log
(\frac 1 {\Delta})$ bits.












\subsection{Examples of Differential Entropy}

\paragraph{Uniform Distribution Entropy: } Let 
\begin{equation}
	f(x) = \begin{cases} 
		\frac{1}{a} & \text{ if } x \in [0,a] \\
		0 			& \text{ else}
	\end{cases}
\end{equation}

Then $h(X) = \log(a)$ (just solve the integral). 


\paragraph{Normal Distribution: } I strongly dislike solving integrals by hand,
so I'm just going to put the result here.
\begin{equation}
	X \sim \phi(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp[\frac{-x^2}{2\sigma^2}]
\end{equation}

\begin{equation}
	h(X) = \frac{1}{2} \log (2\pi e \sigma^2)
\end{equation}






\section{Joint and Conditional Differential Entropy}








\section{AEP for Continuous Variables}

All this essentially follows from the laws of large numbers, just like in the
discrete case. 

\begin{theorem}{AEP for Continuous Random Variables}
Let $X_1, \dots, X_n$ be iid $\sim f(x)$. Then 

\begin{equation}
	\frac{-1}{n} \log f(X_1, \dots, X_n) \to \mathbb E[-\log f(X)] = h(X)
\end{equation}

in probability as $n\to \infty$.

\textbf{Proof sketch:} Weak law of large numbers. The sample entropy converges
to the underlying entropy because $h(X)$ is the expected value of the LHS.
\end{theorem}



Just like in the discrete case, typical sets are all sequences $x^n$ with sample
entropies $\epsilon$-close to the underlying entropy.


\begin{theorem}{Typical Sets for Continuous Random Variables (Definition)}

For any $\epsilon > 0$ and any $n$, the typical set $A_\epsilon^{(n)}$ wrt
$f(x)$ is 

\begin{equation}
	A_\epsilon^{(n)} = \begin{Bmatrix}
		x^n \in S^n : \left|  
			\frac {-1} n \log f(x_1, \dots, x_n) - h(X) 
		\right|
		\leq \epsilon
	\end{Bmatrix}
\end{equation}

Where $S$ is the support set of $f$, $f(x_1, \dots, x^n) = \prod_{i=1}^n
f(x_i)$.
\end{theorem}

Most of the properties of the typical set are the same as in the discrete case.
The main difference is that cardinality $\left| A_\epsilon^{(n)} \right|$ is
replaced by volume $\text{Vol} \left( A_\epsilon ^{(n)} \right)$








\paragraph{Big Takeaways: } 
\begin{itemize}
\item 
\end{itemize}




















\chapter{Annoying Mechanical Procedures} 

Have you ever dreamed of being an automaton? Do you wish that electronic
computers had never been invented so that you could be a professional human
computer? If you answered yes to either of these questions, this chapter is for
you!

\section{Sardinas-Patterson Test for Unique Decodability}

























\end{document}
