\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools} 
\usepackage{bbm}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\inpr{\langle}{\rangle}%

\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

% \def\reals{{\rm I\!R}}
\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}


\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{EE/Ma/CS 126a: Information Theory}
\author{Aman Bhargava}
\date{October-December 2022}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

This document offers an overview of EE/Ma/CS 126a at Caltech. They comprise my condensed course notes for the course. No promises are made relating to the correctness or completeness of the course notes. These notes are meant to highlight difficult concepts and explain them simply, not to comprehensively review the entire course.

\paragraph{Course Information}
\begin{itemize}
\item Professor: Michelle Effros
\item Term: 2022 Fall
\end{itemize}





\chapter{Math Review}


\section{Combinatorics \& Probability}

\paragraph{Binomial Distribution \& Coefficient} 

\begin{itemize}

\item \textbf{Bernoulli Process}: Repeated trials, each with one binary outcome.
The probability of a positive outcome is $p\in [0,1]$. Each trial is
independent.
\item \textbf{Binomial Distribution}: Let $x$ represent the number of successful
trials in a Bernoulli process repeated $n$ times with success probability $p$.
The binomial distribution gives the probability distribution on $x$: 

\begin{equation}\label{eqn:binomial}
b(x; n, p) = {n \choose k} p^x (1-p)^{n-x}
\end{equation}

Which has $\mu = np$, $\sigma^2 = npq$.

\item \textbf{Intuition for Binomial Distribution}: The probability of observing
a sequence with $x$ positive outcomes and $n-x$ negative outcomes is $p^x
(1-p)^{n-x}$. There are ${n\choose k}$ different sequences (i.e., permutations)
that have $x$ positive cases and $n$ negative cases. Thus the total probability
of observing $x$ positive cases is given by Eq~\ref{eqn:binomial}.

\item \textbf{Binomial Coefficient}: 
\begin{equation}
\label{eqn:binomial_coeff}
{n\choose k} = \frac{n!} {k! (n-k)!}
\end{equation}

\end{itemize}


\section{Logarithm Identities}

Entropy calculations and manipulations involve a lot of logarithms. They're not
so bad once you get to know them, though: 
\begin{itemize}
\item \textbf{Definition}: $$a = b^{\log_b a}$$
\item \textbf{Sum-Product}: $$\log_c(ab) = \log_c a + \log_c b$$
\item \textbf{Difference-Quotient}: $$\log (a/c) = \log a - \log c$$ 
$$\log \frac 1 a = - \log a$$
\item \textbf{Product-Exponent}: $$\log_c(a^n) = n\log_c(a)$$
\item \textbf{Swapping Base}: $$\log_b(a) = \log_a(b)$$
\item \textbf{Swapping Exponential}: $$a^{\log n} = n^{\log a}$$
\item \textbf{Change of Base}; $$\log_b(a) = \frac{\log_x(a)}{\log_x(b)}$$
\end{itemize}















\chapter{Entropy Definitions}
\textit{Chapter 2 of Elements of Information Theory}.


\section{Entropy, Conditional Entropy, Joint Entropy}

\paragraph{Entropy Definition (Discrete): } 
\begin{align}\label{eqn:entropy}
H(X) &= \sum_{x\in \mathcal X} p(x) \log ( \frac{1}{p(x)} ) \\
&= - \sum_{x\in \mathcal X} p(x) \log p(x) \\ 
&= \mathbb{E} [\log \frac {1} {p(x)}] 
\end{align}


\begin{theorem}{Properties of Entropy}
\begin{enumerate}
\item \textbf{Non-negativity: } $H(X) \geq 0$ -- \textit{Reasoning: Entropy is the sum-product of
non-negative terms}.
\item \textbf{Change of base: } $H_b(X) = (\log_b a) H_a(X)$

\item \textbf{Bernoulli entropy: } $H(X) = -p\log p - q \log q \equiv H(p)$.
\begin{itemize}
\item $H(p)$ is a concave function of $p$, peaks at $p=q=0.5$.
\end{itemize}
\end{enumerate}
\end{theorem}



\paragraph{Joint Entropy: } Literally just entropy of vector $[X,Y]^\top$.
\begin{align}
\label{eqn:joint_entropy}
H(X, Y) &= - \sum_{x\in \mathcal X} \sum_{y\in \mathcal Y}^{} p(x,y) \log p(x,y) \\ 
&= \mathbb E [\log p(x,y)] \\
\end{align}


\paragraph{Conditional Entropy: } $H(Y|X)$ is the expected entropy of $p(y|x)$
averaged across all $x$.
\begin{align}
\label{eqn:conditional_entropy}
H(Y | X) &= \sum_{x\in \mathcal X} p(x) \underbrace{ \sum_{y\in \mathcal Y} p(y
| x) \log p(y | x) }_{H(Y| X = x)} \\
&= - \mathbb E [\log p(Y | X)]
\end{align}



Entropy can be thought of as the \textbf{uncertainty} in the value of a random
variable. High entropy corresponds to a high degree of uncertainty. Conditional
entropy $H(Y|X)$ can be thought of as the average \textbf{remaining uncertainty}
in the value of $Y$ after learning the value of $X$.


\begin{theorem}{Chain Rule for Entropy}
\begin{align}\label{eqn:simple_entropy_chain}
H(X, Y) &= H(X) + H(Y | X) \\
&= H(Y) + H(X | Y)
\end{align}

It also follows that 
\begin{align}
H(X, Y | Z) &= H(X | Z) + H(Y | X, Z) 
\end{align}


\textbf{Proof sketch:} 
\begin{itemize}
\item Recall that $H(X) = -\mathbb E [\log p(x)]$ and $H(Y | X) = -\mathbb E
[\log p(y | x)]$. 
\item $\log p(x) + \log p(y | x) = \log(p(x) \cdot p(y | x)) = \log p(x,y)$.
\item The proof follows from there. You can also write out the full sum form of
$H(X, Y)$ and recover $H(X), H(Y| X)$ from there if you're feeling rigorous.
\end{itemize}
\end{theorem}





\section{Relative Entropy \& Mutual Information}

\paragraph{Relative Entropy: } $D(p \| q)$ gives a \textit{distance} between
distributions $p(x)$ and $q(x)$. Also known as KL divergence. 
\begin{align}
\label{eq:relative_entropy}
D(p \| q) &= \sum_{x\in \mathcal X} p(x) \log \frac{p(x)}{q(x)} \\ 
&= \mathbb E_{p(x)} [\log \frac {p(x)}{q(x)}] \\ 
\end{align}

This also corresponds to the \textbf{inefficiency} of using $q$ as a replacement
for $p$ when generating codes for tokens drawn from $p(x)$. 
\begin{itemize}
\item \textbf{Average code length with correct $p(x)$:} $H(p)$.
\item \textbf{Average code length with incorrect $q(x)$:} $H(p) + D(p \| q)$.
\end{itemize}

\begin{theorem}{Properties of Relative Entropy}
\begin{enumerate}
\item \textbf{Asymmetric: } In general, $D(p\| q) \neq D(q \| p)$.
\item \textbf{Non-negative: } $D(p\| q) \geq 0$.
\item \textbf{Identity: } If $D(p\| q) = 0$ then $p \equiv q$.
\end{enumerate}
\end{theorem}



We now move onto \textbf{mutual information} -- a measure of the dependence of
two variables. As we will see, it is the \textbf{reduction in uncertainty} of
$X$ due to knowing $Y$, on average. 

\paragraph{Mutual Information: } \begin{align}
\label{eqn:mutual_info}
I(X; Y) &= \sum_{x\in \mathcal X}^{} \sum_{y\in \mathcal Y}^{} p(x, y) \log
(\frac{p(x,y)}{p(x)p(y)} ) \\ 
&= D\big(p(x,y) \| p(x)p(y) \big) \\
&= \mathbb E [\log \frac{p(X,Y)}{p(X)p(Y)}]
\end{align}

\begin{theorem}{Properties of Mutual Information}
\begin{itemize}
\item It is the \textbf{divergence} between $p(x,y)$ and $p(x)p(y)$.
\item \textbf{Symmetry: } $I(X; Y) = I(Y; X)$.
\item \textbf{Relation to Entropy: } Mutual information is the \textit{reduction
in uncertainty} of each RV expected after discovering the other variable's
value. 
	\begin{align}
		\label{eqn:mi_entropy}
		I(X; Y) &= H(X) - H(X | Y) \\ 
				&= H(Y) - H(Y | X) \\
	\end{align}

\item \textbf{Alternative Entropy Relation: }
	\begin{align}
		\label{eqn:mi_entropy_2}
		I(X; Y) &= H(X) + H(Y) - H(X, Y) \\ 
		I(X; X)	&= H(X) - H(X | X) \\ 
				&= H(X)
	\end{align}

\end{itemize}



\textbf{Proof Sketch for (3): } 
\begin{itemize}
\item Within the definition of $I(X; Y)$ there is a term $\log \frac{p(x,y)}{p(x)p(y)}$. 
\item Once you convert the argument of the log into $p(x | y) / p(x)$, you can separate out $H(X) - H(X | Y)$ using the
quotient-difference logarithm rule.
\end{itemize}




\section{Chain Rules: $H(\cdot), I(\cdot ; \cdot)$}

These chain rules end up being very useful in a lot of proofs. Deeply
understanding them is a good idea. 

\begin{theorem}{Entropy chain rule}
Let $X_1, X_2, \dots, X_n \tilde p(x_1, x_2, \dots, x_n)$. Then 
\begin{align}
	\label{eqn:entropy_chain_rule}
	H(X_1, \dots, X_n) &= \sum_{i=1}^{n} H(X_i | X_{i-1}, X_{i-2}, \dots, X_1)
\end{align}
\textbf{Proof: } Repeatedly apply Equation~\ref{eqn:simple_entropy_chain}.
\end{theorem}
\paragraph{Intuition of Chain Rule: } It's important to note that the term in
the sum is conditioned on elements $X_j$ with $j < i$. Conditioning always
reduces entropy, so it's as though the ``additional entropy'' from the term must
be reduced to account for the previous terms already having been added to the
total. Also note that any order can suffice -- there is no absolute order in the
sum.



\end{theorem}









































\end{document}
