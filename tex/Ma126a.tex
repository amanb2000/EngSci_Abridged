\documentclass[a4paper,12pt]{report}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subfig}
\usepackage{listings}
\usepackage{media9}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{mathtools} 
\usepackage{bbm}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%
\DeclarePairedDelimiter\inpr{\langle}{\rangle}%

\newcommand*{\horzbar}{\rule[.5ex]{2.5ex}{0.5pt}}

% \def\reals{{\rm I\!R}}
\def\reals{\mathbb{R}}
\def\integers{\mathbb{Z}}


\newtheorem{theorem}{Theorem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\begin{document}

\title{EE/Ma/CS 126a: Information Theory}
\author{Aman Bhargava}
\date{October-December 2022}
\maketitle

\tableofcontents

\section{Introduction and Course Information}

This document offers an overview of EE/Ma/CS 126a at Caltech. They comprise my condensed course notes for the course. No promises are made relating to the correctness or completeness of the course notes. These notes are meant to highlight difficult concepts and explain them simply, not to comprehensively review the entire course.

\paragraph{Course Information}
\begin{itemize}
\item Professor: Michelle Effros
\item Term: 2022 Fall
\end{itemize}





\chapter{Math Review}


\section{Combinatorics \& Probability}

\paragraph{Binomial Distribution \& Coefficient} 

\begin{itemize}

\item \textbf{Bernoulli Process}: Repeated trials, each with one binary outcome.
The probability of a positive outcome is $p\in [0,1]$. Each trial is
independent.
\item \textbf{Binomial Distribution}: Let $x$ represent the number of successful
trials in a Bernoulli process repeated $n$ times with success probability $p$.
The binomial distribution gives the probability distribution on $x$: 

\begin{equation}\label{eqn:binomial}
b(x; n, p) = {n \choose k} p^x (1-p)^{n-x}
\end{equation}

Which has $\mu = np$, $\sigma^2 = npq$.

\item \textbf{Intuition for Binomial Distribution}: The probability of observing
a sequence with $x$ positive outcomes and $n-x$ negative outcomes is $p^x
(1-p)^{n-x}$. There are ${n\choose k}$ different sequences (i.e., permutations)
that have $x$ positive cases and $n$ negative cases. Thus the total probability
of observing $x$ positive cases is given by Eq~\ref{eqn:binomial}.

\item \textbf{Binomial Coefficient}: 
\begin{equation}
\label{eqn:binomial_coeff}
{n\choose k} = \frac{n!} {k! (n-k)!}
\end{equation}

\end{itemize}


\section{Logarithm Identities}

Entropy calculations and manipulations involve a lot of logarithms. They're not
so bad once you get to know them, though: 
\begin{itemize}
\item \textbf{Definition}: $$a = b^{\log_b a}$$
\item \textbf{Sum-Product}: $$\log_c(ab) = \log_c a + \log_c b$$
\item \textbf{Difference-Quotient}: $$\log (a/c) = \log a - \log c$$ 
$$\log \frac 1 a = - \log a$$
\item \textbf{Product-Exponent}: $$\log_c(a^n) = n\log_c(a)$$
\item \textbf{Swapping Base}: $$\log_b(a) = \log_a(b)$$
\item \textbf{Swapping Exponential}: $$a^{\log n} = n^{\log a}$$
\item \textbf{Change of Base}; $$\log_b(a) = \frac{\log_x(a)}{\log_x(b)}$$
\end{itemize}















\chapter{Entropy Definitions}
\textit{Chapter 2 of Elements of Information Theory}.


\section{Entropy, Conditional Entropy, Joint Entropy}

\paragraph{Entropy Definition (Discrete): } 
\begin{align}\label{eqn:entropy}
H(X) &= \sum_{x\in \mathcal X} p(x) \log ( \frac{1}{p(x)} ) \\
&= - \sum_{x\in \mathcal X} p(x) \log p(x) \\ 
&= \mathbb{E} [\log \frac {1} {p(x)}] 
\end{align}


\begin{theorem}{Properties of Entropy}
\begin{enumerate}
\item \textbf{Non-negativity: } $H(X) \geq 0$ -- \textit{Reasoning: Entropy is the sum-product of
non-negative terms}.
\item \textbf{Change of base: } $H_b(X) = (\log_b a) H_a(X)$

\item \textbf{Bernoulli entropy: } $H(X) = -p\log p - q \log q \equiv H(p)$.
\begin{itemize}
\item $H(p)$ is a concave function of $p$, peaks at $p=q=0.5$.
\end{itemize}
\end{enumerate}
\end{theorem}



\paragraph{Joint Entropy: } Literally just entropy of vector $[X,Y]^\top$.
\begin{align}
\label{eqn:joint_entropy}
H(X, Y) &= - \sum_{x\in \mathcal X} \sum_{y\in \mathcal Y}^{} p(x,y) \log p(x,y) \\ 
&= \mathbb E [\log p(x,y)] \\
\end{align}


\paragraph{Conditional Entropy: } $H(Y|X)$ is the expected entropy of $p(y|x)$
averaged across all $x$.
\begin{align}
\label{eqn:conditional_entropy}
H(Y | X) &= \sum_{x\in \mathcal X} p(x) \underbrace{ \sum_{y\in \mathcal Y} p(y
| x) \log p(y | x) }_{H(Y| X = x)} \\
&= - \mathbb E [\log p(Y | X)]
\end{align}



Entropy can be thought of as the \textbf{uncertainty} in the value of a random
variable. High entropy corresponds to a high degree of uncertainty. Conditional
entropy $H(Y|X)$ can be thought of as the average \textbf{remaining uncertainty}
in the value of $Y$ after learning the value of $X$.


\begin{theorem}{Chain Rule for Entropy}
\begin{align}\label{eqn:simple_entropy_chain}
H(X, Y) &= H(X) + H(Y | X) \\
&= H(Y) + H(X | Y)
\end{align}

It also follows that 
\begin{align}
H(X, Y | Z) &= H(X | Z) + H(Y | X, Z) 
\end{align}


\textbf{Proof sketch:} 
\begin{itemize}
\item Recall that $H(X) = -\mathbb E [\log p(x)]$ and $H(Y | X) = -\mathbb E
[\log p(y | x)]$. 
\item $\log p(x) + \log p(y | x) = \log(p(x) \cdot p(y | x)) = \log p(x,y)$.
\item The proof follows from there. You can also write out the full sum form of
$H(X, Y)$ and recover $H(X), H(Y| X)$ from there if you're feeling rigorous.
\end{itemize}
\end{theorem}





\section{Relative Entropy \& Mutual Information}

\paragraph{Relative Entropy: } $D(p \| q)$ gives a \textit{distance} between
distributions $p(x)$ and $q(x)$. Also known as KL divergence. 
\begin{align}
\label{eq:relative_entropy}
D(p \| q) &= \sum_{x\in \mathcal X} p(x) \log \frac{p(x)}{q(x)} \\ 
&= \mathbb E_{p(x)} [\log \frac {p(x)}{q(x)}] \\ 
\end{align}

This also corresponds to the \textbf{inefficiency} of using $q$ as a replacement
for $p$ when generating codes for tokens drawn from $p(x)$. 
\begin{itemize}
\item \textbf{Average code length with correct $p(x)$:} $H(p)$.
\item \textbf{Average code length with incorrect $q(x)$:} $H(p) + D(p \| q)$.
\end{itemize}

\begin{theorem}{Properties of Relative Entropy}
\begin{enumerate}
\item \textbf{Asymmetric: } In general, $D(p\| q) \neq D(q \| p)$.
\item \textbf{Non-negative: } $D(p\| q) \geq 0$.
\item \textbf{Identity: } If $D(p\| q) = 0$ then $p \equiv q$.
\end{enumerate}
\end{theorem}



\paragraph{Conditional Relative Entropy/KL Divergence: } Distance between two
distributions when conditioned on the same variable. Similar idea of averaging
across all values of the conditioning variable. 
\begin{align}\label{eqn:conditional_relative_entropy}
D(p(y|x) \| q(y | x)) &= \sum_{x\in \mathcal X}^{} p(x) \big[ \sum_{y\in \mathcal
Y}^{} p(y|x) \log \frac{p(y | x)}{q(y|x)}\big] \\
&= \mathbb E_{p(x,y)} \log \big[ \frac{p(y|x)}{q(y|x)}\big]
\end{align}







We now move onto \textbf{mutual information} -- a measure of the dependence of
two variables. As we will see, it is the \textbf{reduction in uncertainty} of
$X$ due to knowing $Y$, on average. 

\paragraph{Mutual Information: } \begin{align}
\label{eqn:mutual_info}
I(X; Y) &= \sum_{x\in \mathcal X}^{} \sum_{y\in \mathcal Y}^{} p(x, y) \log
(\frac{p(x,y)}{p(x)p(y)} ) \\ 
&= D\big(p(x,y) \| p(x)p(y) \big) \\
&= \mathbb E [\log \frac{p(X,Y)}{p(X)p(Y)}]
\end{align}




\begin{theorem}{Properties of Mutual Information}
\begin{itemize}
\item It is the \textbf{divergence} between $p(x,y)$ and $p(x)p(y)$.
\item \textbf{Symmetry: } $I(X; Y) = I(Y; X)$.
\item \textbf{Relation to Entropy: } Mutual information is the \textit{reduction
in uncertainty} of each RV expected after discovering the other variable's
value. 
	\begin{align}
		\label{eqn:mi_entropy}
		I(X; Y) &= H(X) - H(X | Y) \\ 
				&= H(Y) - H(Y | X) \\
	\end{align}

\item \textbf{Alternative Entropy Relation: }
	\begin{align}
		\label{eqn:mi_entropy_2}
		I(X; Y) &= H(X) + H(Y) - H(X, Y) \\ 
		I(X; X)	&= H(X) - H(X | X) \\ 
				&= H(X)
	\end{align}

\end{itemize}

\textbf{Proof Sketch for (3): } 
\begin{itemize}
\item Within the definition of $I(X; Y)$ there is a term $\log \frac{p(x,y)}{p(x)p(y)}$. 
\item Once you convert the argument of the log into $p(x | y) / p(x)$, you can separate out $H(X) - H(X | Y)$ using the
quotient-difference logarithm rule.
\end{itemize}
\end{theorem}


\paragraph{Conditional Mutual Information: } $I(X, Y | Z)$ is the average
reduction in uncertainty on the value of $X$ due to knowing $Y$ when $Z$ is
given. 
\begin{align}
	\label{eqn:cond_MI}
	I(X; Y | Z) &= H(X | Z) - H(X | Y, Z) \\ 
				&= \mathbb E_{p(x,y,z)}	\log \big[ \frac{p(X, Y | Z)}{p(X|Z)
				p(Y|Z)} big] \\
\end{align}








\section{Chain Rules: $H(\cdot), I(\cdot ; \cdot)$}

These chain rules end up being very useful in a lot of proofs. Deeply
understanding them is a good idea. 

\begin{theorem}{Entropy chain rule}
\label{thm:entropy_chain_rule}
Let $X_1, X_2, \dots, X_n \tilde p(x_1, x_2, \dots, x_n)$. Then 
\begin{align}
	\label{eqn:entropy_chain_rule}
	H(X_1, \dots, X_n) &= \sum_{i=1}^{n} H(X_i | X_{i-1}, X_{i-2}, \dots, X_1)
\end{align}
\textbf{Proof: } Repeatedly apply Equation~\ref{eqn:simple_entropy_chain}.
\end{theorem}

\paragraph{Intuition of Chain Rule: } It's important to note that the term in
the sum is conditioned on elements $X_j$ with $j < i$. Conditioning always
reduces entropy, so it's as though the ``additional entropy'' from the term must
be reduced to account for the previous terms already having been added to the
total. Also note that any order can suffice -- there is no absolute order in the
sum.



\begin{theorem}{Chain Rule for Mutual Information}
\begin{align}
	\label{eqn:chain_rule_MI}
	I(X_1, \dots, X_n; Y) &= \sum_{i=1}^{n} I(X_i; Y | X_{i-1}, X_{i-2}, \dots,
	X_1) \\
\end{align}
\textbf{Proof: } Start with the entropy definition of mutual information. Then
apply the chain rule for entropy (Theorem~\ref{thm:entropy_chain_rule}).
\begin{itemize}
\item $I(X_{1:n} ; Y) = H(X_{1:n}) - H(X_{1:n} | Y)$.
\item $= \sum_{i=1}^{n} H(X_i | X_{i-1:1}) - \sum_{i=1}^{n} H(X_i | X_{i-1:1},
Y)$.
\item $= \sum_{i=1}^{n} I(X_i; Y | X_{1:i-1})$.
\end{itemize}
\end{theorem}





\begin{theorem}{Chain Rule for Relative Entropy}
\begin{equation}
D(p(x,y) \| q(x,y)) = D(p(x) \| q(x)) + D(p(y | x) \| q(y|x))
\end{equation}

\textbf{Proof sketch:} Expand the LHS in log-sum form. Separate the term in the
$\log$ into a sum of two $\log$ terms corresponding to the two divergences on
the RHS. 

\end{theorem}



\section{Jensen's Inequality \& Consequences}

\begin{theorem}{Jensen's Inequality (and Convexity)}
For any convex function $f$ and random variable $X$, 
\begin{align}
	\mathbb E \big[ f(x)\big] \geq f(\mathbb E[X])
\end{align}

Where ``convex $f$ in $(a,b)$'' $\iff$ 
\begin{align}
	\label{eqn:convexity}
	f(\lambda x_1 + (1-\lambda) x_2 ) \leq \lambda f(x_1) + (1-\lambda) f(x_2)
\end{align}
for all $x_1, x_2 \in (a, b)$ and $\lambda \in [0,1]$. \textbf{Strict} convexity
has equality iff $\lambda \in \{0,1\}$. \textbf{Concave} $f$ $\iff$ convex $-f$.


\textbf{Proof sketch of Jensen's Inequality: } 
\begin{itemize}
\item Start with $|\mathcal X| = 2$. Then we can let a vector $\mathbf p\in
\mathbb R^2$ represent f(x). 
\item $\mathbb E[f(x)] = p_1 f(x_1) + p_2 f(x_2)$. 
\item $f(\mathbb E[X]) = f(p_1 x_1 + p_2 x_2)$.
\item Show equivalence of Jensen's inequality $\iff$ $\mathbf p$ is a valid PMF.
\item Expand to $|\mathcal X| = k-1$ (induction proof).
\item Use continuity arguments for continuous case.
\end{itemize}
\end{theorem}


\begin{theorem}{Implications of Jensen's Inequality}
\begin{enumerate}
\item \textbf{Information Inequality: } $D(p\| q) \geq 0$.
\item \textbf{MI Inequality: } $I(X; Y) \geq 0$.
\item \textbf{Maximum Entropy: } $H(X) \leq \log |\mathcal X|$. Maximum is
achieved by $X\tilde {uniform} (\mathcal X)$.
\item \textbf{Information can't Hurt: } $H(X|Y) \leq H(X)$.
\item \textbf{Entropy Sum Bound: } $H(X_{1:n}) \leq \sum_{i=1}^{n} H(X_i)$ with
equality for independent $X_i$.
\end{enumerate}
\end{theorem}



\section{Log-Sum Inequality \& Consequences}

\begin{theorem}{Log-Sum Inequality}
Let $\{a_i, b_i\}_{i=1}^n$ be \textbf{non-negative} numbers. Then 
\begin{align}
\sum_{i=1}^{n} a_i \log \frac{a_i}{b_i} \geq \big(\sum_{i=1}^{n} a_i \big) \log
\frac{\sum a_i}{\sum b_i} 
\end{align}
With equality iff $\frac{a_i}{b_i}$ is constant.

\textbf{Proof sketch: } 
\begin{enumerate}
\item Introduce $\alpha_i = a_i/\sum a_j$ and $t_i = a_i/b_i$. $\alpha_i$ values
comprise a probability distribution (sum to 1). 
\item Apply \textbf{Jensen's} to $\sum \alpha_i f(t_i) \geq f(\sum \alpha_i
t_i)$ where $f() = \log()$.
\end{enumerate}
\end{theorem}




\begin{theorem}{Applications of Log-Sum Inequality}

\begin{itemize}
\item \textbf{Convexity of Relative Entropy: } $D(p\|q)$ is convex in pairs of
$p, q$.
	\begin{align}
		D(\lambda p_1 + (1-\lambda) p_1 \| \lambda q_1 + (1-\lambda) q_2) \leq
		\lambda D(p_1 \| q_1) + (1-\lambda) D(p_2 \| q_2)
	\end{align}

\item \textbf{Concavity of Mutual Information: } 
\begin{enumerate}
\item \textbf{For fixed $p(y|x)$:} $I(X; Y)$ is concave in $p(x)$.
\item \textbf{For fixed $p(x)$:} $I(X; Y)$ if concave in $p(y | x)$.
\item \textit{This property is really handy when doing channel capacity
calculations/bounds!} 
\end{enumerate}
\end{itemize}


\end{theorem}









\section{Data Processing Inequality}

\begin{theorem}{Data Processing Inequality}
Let $X \to Y \to Z$ form a markov chain. That is, $p(x,y,z) = p(x)p(y|x)p(z|y)$.
Then 
\begin{equation}
I(X; Y) \geq I(X; Z)
\end{equation}

That is, the mutual information between $X, Z$ is bounded by the mutual
information between $X, Y$.

\textbf{Proof sketch:} 
\begin{itemize}
\item Expand $I(X; Y, Z)$ with the chain rule.
\item ($\star$) Observe that $I(X; Z | Y) = 0$ since $X, Z$ are \textbf{conditionally
independent} given $Y$ (recall Markov theory -- head-to-tail connection).
\item Since $I(X; Y | Z) \geq 0$, we can conclude that $I(X; Y) \geq I(X; Z)$.
\end{itemize}
\end{theorem}


\paragraph{Sufficient Statistics Connection: } Assume that $X \to Y$ is a Markov
chain. E.g., $X$ are some parameters of an underlying distribution and $Y$ are
some data collected from the distribution. If $Z = f(Y)$, then we have $X \to Y
\to Z$. Therefore any function $Z$ on the data $Y$ cannot have greater
information on the underlying distribution $X$ than $Y$ did in the first place. 

A \textbf{sufficient statistic} is one that has all the information that the
data had about the underlying distribution. That is, $Z = f(Y)$ is a sufficient
statistic on $Y$ if $I(Z; X) = I(Y; X)$. 

\paragraph{Minimal Sufficient Statistic: } Let $\theta \to T(X) \to U(X) \to X$.
$T(X)$ is the \textit{minimal sufficient statistic} iff $U(X)$ can be any
sufficient statistic and still have $\theta \to T(X) \to U(X) \to X$ be a valid
Markov chain. 





\section{Fano's Inequality}

Fano's inequality concerns estimators for random variables. Given some
correlated random variables $X, Y$, we want to understand the probability of
error when using an estimator $\hat X(Y)$ to approximate $X$. The big reveal is
that $\Pr(\text{error}) = \text{function}(H(X|Y))$.


\begin{theorem}{Fano's Inequality}
Let $X,Y$ be dependent random variables. $\hat X(Y)$ is an estimator on $X$, so
$X\to Y\to \hat X$ is a valid Markov chain for the joint distribution. Then 
\begin{align}
	\label{eqn:fano}
	H(P_{err}) + P_{err} \log |\mathcal X| \geq H(X | \hat X) \geq H(X | Y)
\end{align}

A weaker form of the same being: 
\begin{align}
	\label{eqn:fano_weak}
	1 + P_{err} \log |\mathcal X| &\geq H(X | Y) \\
	P_{err} &\geq \frac{H(X| Y)-1}{\log |\mathcal X|} \\
\end{align}

\textbf{Proof Sketch:} 
\begin{itemize}
\item Represent the ``error'' event as random variable $E$. It takes value 1 if
$\hat X \neq X$ and zero if $\hat X = X$.
\item $P_{err} = \mathbb E[E]$.
\item Expand $H(E, X | \hat X) = H(X | \hat X) + H(E | X, \hat X)$, noting that
the last term must be zero.
\item Also note that $H(E | \hat X) \leq H(E)$ (conditioning reduces entropy).
\item $H(X | E, \hat X)$ can be expanded and shown to be bounded by $P_e \log
|\mathcal X|$.
\item Finally use Markov's inequality for $H(X | \hat X) \geq H(X | Y)$. 
\item Combine everything and you should be able to get the strongest version in
Equation~\ref{eqn:fano}.
\end{itemize}
\end{theorem}

You can also strengthen it to $$H(P_{err}) + P_{err} \log (\underbrace{|\mathcal
X| - 1}_{(\star)}) \geq H(X|Y)$$ since one element of $\mathcal X$ is eliminated
from the error entropy calculation by random guessing.


\paragraph{``Sharpness'' of Fano's Inequality: } If you have no information $Y$
to inform $\hat X$, your best guess is $\hat X = \arg\max_x p(x)$. Equality in
Fano's inequality (i.e., $H(P_{err}) + P_{err} \log(|\mathcal X| - 1) \geq
H(X)$) is achieved by $p(\hat x) = 1-P_{err}$ and $p(x \neq \hat x) =
P_{err}/(|\mathcal X|-1)$.


\paragraph{Bound on Collision: } Let $X, X' \tilde p(x)$. Then $\Pr\{X = X'\} =
\sum_{x\in \mathcal X}^{} (p(x))^2$. Then \begin{equation}
\Pr(X = X') \geq 2^{-H(X)}
\end{equation}

with equality if $X\tilde unif(\mathcal X)$. 

\textbf{Proof sketch:} Expand RHS with entropy in $\mathbb E[]$ form. Apply
Jensen's inequality, and you'll recover $\sum p(x)^2$ as the upper bound. 


\paragraph{Collisions with Different Distributions: } Let $X\tilde p(x)$ and
$\hat X \tilde r(x)$. Then 
\begin{align}
	\Pr(X\neq \hat X) &\geq 2^{-H(p) - D(p\|r)} \\
	\Pr(X\neq \hat X) &\geq 2^{-H(r) - D(r\|p)} \\
\end{align}
\textbf{Proof sketch:} We know the LHS is $\sum_x p(x)r(x)$. Expanding the RHS,
we can apply Jensen's inequality if $H$ and $D$ are in $\mathbb E$ form. Then we
will have a bound equal to LHS.










\chapter{Asymptotic Equipartition Theorem (AEP)}

\section{Typicality and AEP Theorem}

AEP is the application of the \textbf{weak law of large numbers} (WLLN) to
entropy.  Recall WLLN: 
\begin{equation}
	\label{eqn:wlln}
	\lim_{n\to \infty} \frac{1 }{n} \sum_{i=1}^{n} X_i = \mathbb E[X]
\end{equation}

Applying to entropy, 

\begin{theorem}{Asymptotic Equipartition Theorem}
Let $X_1 \dots X_n$ be iid with PMF $p(x)$. Then 
\begin{align}
	\label{eqn:aep}
	\frac{1}{n} \log \frac{1}{p(X_1, \dots, X_n)} &\to H(x) \\
	p(X_1, \dots, X_n) &\to 2^{-n H(x)}
\end{align}

in probability. That is, for all $\epsilon > 0$, $\exists$ $n$ such that the
difference between the sample entropy (LHS) and the true entropy (RHS) is less
than $\epsilon$.

\textbf{Proof sketch:} Apply WLLN to the definition of entropy. LHS = RHS in
limit $n\to \infty$..
\end{theorem}


\paragraph{Typical sequences: } Sequences of $x_i$ with \textbf{sample entropy}
$\approx nH(X)$. 

\paragraph{Typical set $A_\epsilon^{(n)}$ w.r.t. $p(x)$: } A set of sequences
$x^n\in \mathcal X^n$ that satisfy 
\begin{align}
	\label{eqn:typical_set}
	2^{-n(H(X) + \epsilon)} \leq p(x_1, \dots, x_n) \leq 2^{-n(H(X) - \epsilon)}
\end{align}


\begin{theorem}{Properties of $A_\epsilon^{(n)}$}
\begin{enumerate}
\item $x^n \in A_\epsilon^{(n)}$ $\to$ sample entropy $- \frac 1 n \log p(x^n)
\in [H(X)-\epsilon, H(X)+\epsilon]$.
\item $\Pr\{X^n \in A_\epsilon^{(n)}\} = \Pr\{A_\epsilon^{(n)}\} \geq
1-\epsilon$ for large $n$.
\item $|A_\epsilon^{(n)}| \leq 2^{n(H(x) + \epsilon)}$ for all $n$.
\item $A_\epsilon^{(n)} \geq (1-\epsilon) 2^{n(H(x) - \epsilon)}$ for
sufficiently large $n$.
\end{enumerate}

\textbf{Proof Sketches:} 
\begin{enumerate}
\item Rearrangement of AEP/definition of $A_\epsilon^{(n)}$.
\item Apply the lower bound on $\Pr{x^n}: x^n\in A_\epsilon^{(n)}$.
\item $\Pr\{A_\epsilon^{(n)}\} \leq 1$. But we also know that $\Pr(x^n) \geq
2^{-n(H(x) + \epsilon)}$ if $x^n \in A_\epsilon^{(n)}$. Do the math.
\item $\Pr\{A_\epsilon^{(n)}\} \geq 1-\epsilon$ (from 2). Since typical
sequences $x^n$ have maximum probability $2^{-n(H(X)-\epsilon)}$, we can derive
this bound.
\end{enumerate}

\end{theorem}





\chapter{Data Compression}

\textit{Tail end of EIT chapter 3 and the entirety of chapter 5}.


\paragraph{Data Compression -- Problem Statement: } We want to find the
\textit{shortest} codes for transmitting sequences $x^n$ where each token is iid
with PMF $p(x)$. 
\begin{itemize}
\item We can leverage notions of \textbf{typicality} since we know that $X^n \in
A_\epsilon^{(n)}$ with arbitrarily high probability for large $n$.
\item \textbf{Simple implementation:} Introduce some ordering to elements in
$A_\epsilon^{(n)}$. Since the size of $A_\epsilon^{(n)} \approx 2^{nH(X)}$, a
binary index would need $n(H+\epsilon) + 1$ bits. This is a pretty good code
already!
\item For items not in $A_\epsilon^{(n)}$, we can prepend a flag bit and use
codes of length $n \log |\mathcal X| + 1$.
\end{itemize}




\section{Definitions \& Source Coding Theorem}

\paragraph{Compression/Source Coding Preliminaries: } 
\begin{itemize}
\item \textbf{Source:} Random variable $X: x\in \mathcal X$.
\item \textbf{Codeword Alphabet:} $\mathcal D$ is a $D$-ary alphabet.
\item \textbf{Source Code $C: \mathcal X \to \mathcal D^*$ } maps from the
source alphabet to strings of arbitrary length from the code alphabet. 
\item \textbf{Expected Length $L(C) = \sum_{x\in \mathcal X} p(x)\ell(x)$} where
$\ell(x) = |C(x)|$.
\item \textbf{Assumption: } We tend to represent every $D$-ary alphabet with
natural numbers $\{0,1,\dots, 1-D\}$.
\item \textbf{Non-singular: } Code $C$ is non-singular iff \begin{align}
	\label{eqn:non_sing}
	x_1 \neq x_t \implies C(x_1) \neq C(x_2)
\end{align}
\item \textbf{Extension Code: } $C^*$ is the extension of $C$ -- \begin{align}
C^*(x_1, x_2, \dots) = C(x_1)C(x_2)\dots
\end{align}
I.e., the concatenation of $C(x_i)$.
\item \textbf{Uniquely Decodable: } $C$ is UD if $C^*$ is non-singular.
\item \textbf{Prefix Code: } No codeword $c(x_1)$ is a prefix of another
codeword $c(x_2)$. This also means one can decode without any future information
-- it is an \textbf{instantaneous code}.
\end{itemize}






\begin{theorem}{Source Coding Theorem}
Let $X^n$ be iid with each $X_i \tilde p(x)$. Then \textbf{there exists} a code
with lengths satisfying: 
\begin{align}
	\label{eqn:source_coding_theorem}
	\mathbb E\big[ \frac{1}{n} \ell(X^n) \big] \leq H(X) + \epsilon
\end{align}
for $n$ sufficiently large. In other words, we can represent sequences of length
$n$ using $nH(X)$ bits on average!

\textbf{Proof sketch:} 
\begin{itemize}
\item For large $n$, $\Pr\{A_\epsilon^{(n)}\} \to 1$.
\item Then $\mathbb E[\ell(X^n)] = \sum_{x^n\in \mathbb X^n} p(x^n) \ell(x^n)$.
\item Split the sum into $x^n \in A_\epsilon^{(n)}$ and the compliment.
\item Apply codes of length $n(H+\epsilon) + 2$ for the first sum and lengths $n
\log |\mathcal X| + 2$ to the second (1 flag bit, 1 rounding bit).
\item Simplify using $\Pr\{A_\epsilon^{(n)}\}$ and upper bounds on
$A_\epsilon^{(n)}$ size.
\end{itemize}
\end{theorem}





\section{Kraft Inequality}

The Kraft inequality answers the question, ``how short can codes get?''

\begin{theorem}{Kraft Inequality}
Let $C:\mathcal X^n \to \mathcal D^*$ be a \textbf{prefix code}. Let
$\{\ell_i\}_{i=1}^m$ be the codeword lengths for each $x^n \in \mathcal X^n$
(i.e., $m = |\mathcal X^n|$. Then 

\begin{align}
	\label{eqn:kraft}
	\sum_{i=1}^{m} D^{-\ell_i} \leq 1
\end{align}

And conversely: For any $\{\ell_i\}$ satisfying the inequality, \textbf{there
exists a prefix code with those lengths}!

\textbf{Proof sketch:} 
\begin{itemize}
\item Consider a tree structure for all possible $\mathcal D^*$. Each layer adds
one character, etc. 
\item At the deepest layer $\ell$, there are $D^\ell$ leaf nodes.
\item Each non-leaf node on layer $\ell_i$ has $D^{\ell - \ell_i}$ descendants.
\textbf{To maintain prefix quality}, all descendants are eliminated!
\item The number of descendants on the final layer must sum to $D^{\ell_max}$. 
\item Manipulate these equations and the inequality will pop out :)
\end{itemize}
\end{theorem}


\begin{theorem}{Extended Kraft Inequality}
Even for an infinite prefix code (i.e., countably infinite codewords), the
lengths $\{\ell_i\}_{i=1}^\infty$ will satisfy 
\begin{equation}
	\sum_{i=1}^{\infty} D^{-\ell_i} \leq 1
\end{equation}

\textbf{Proof sketch:} Apply analogy to floating point numbers/place value.
"Descendants" represent intervals, they must be disjoint, etc. 
\end{theorem}



\section{Finding Optimal Codes}

We learned from the Kraft Inequality (Equation~\ref{eqn:kraft}) that the
codeword lengths are constrained to follow $\sum D^{-\ell_i} \leq 1$. Optimal
codes will therefore solve the following optimization problem:
\begin{align}
\min_{\{\ell_i\}} & \sum_{i=1}^{m} p_i \ell_i \\ 
s.t. & \sum_{i=1}^{m} D^{-\ell_i} \leq 1
\end{align}


\paragraph{Lagrange Multiplier Solution: } $\ell_i^* = -\log_D p_i$ satisfies
Kraft inequality and minimizes $\mathbb E[\ell_i]$ -- expected code length
converges to $H_D(x)$. However, we need to round off code lengths so they're
integers!

\begin{theorem}{Expected code length inequality}
For a $D$-ary code and random variable $X: x\in \mathcal X$, 
\begin{align}
	L \geq H_D(X)
\end{align}
with equality iff $D^{-\ell_i} = p_i$ where $p_i = p(x_i)$.

\textbf{Proof sketch:} Start by expanding $L - H_D(X)$. Combine the sums using
product-sum log rule and recover $L - H_D(X) = D(\mathbf p \| \mathbf r)$ where
$r_i = D^{-\ell_i}/\sum_j D^{-\ell_j}$. By non-negativity of $D(\|)$, the proof
is done.
\end{theorem}



\paragraph{D-adic Distributions: } $p(x)$ is D-adic if $\exists \{n_i\}$ such
that \begin{equation}
p(x_i) = D^{-n_i}
\end{equation}
where each $n_i$ is a natural number.


Generally, we want a $D$-adic distribution $p(x)$ so that our codes achieve
expected length $L = \sum p_i \ell_i = H_D(X)$. Failing that, we want to
\textbf{minimize $D(d-adic \| p(x))$}. We are approximating $p(x)$ with the
closet $D$-adic distribution. That's really all that coding methods boil down
to!
\begin{itemize}
\item Shannon-Fano: Offers good, easy, suboptimal codes. 
\item Huffman: Truly optimal codes based on $p(x)$ -- we actually find the
nearest $D$-adic distribution!
\end{itemize}









\section{Sardinas-Patterson Test for Unique Decodability}

























\end{document}
